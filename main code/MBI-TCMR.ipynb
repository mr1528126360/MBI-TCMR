{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.impute import SimpleImputer\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fingerprints(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(fingerprints, self).__init__()\n",
    "        self.fc1 = nn.Linear(325,204) # 全连接层\n",
    "        #print(self.fc1.weight.shape)\n",
    "        self.fc2 = nn.Linear(204,13963) # 全连接层\n",
    "        self.fc3 = nn.Linear(13963,11230) # 全连接层\n",
    "        self.fc4 = nn.Linear(11230,20861) # 全连接层\n",
    "        self.fc5 = nn.Linear(20861,4608) # 全连接层\n",
    "        self.fc6 = nn.Linear(4608,1) # 全连接层\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"0:\"+ str(x.size())) # 输出tensor大小\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(\"1:\"+ str(x.size())) # 输出tensor大小\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #print(\"2:\"+ str(x.size())) # 输出tensor大小\n",
    "        x = F.relu(self.fc3(x))\n",
    "        #print(\"3:\"+ str(x1.size())) # 输出tensor大小\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        #x = F.relu(self.fc6(x))\n",
    "        x = F.softplus(self.fc6(x))\n",
    "        #prelu = torch.nn.PReLU(num_parameters=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2800    1.000000\n",
       "4429    0.578947\n",
       "3583    0.500000\n",
       "7915    0.071429\n",
       "455     0.347826\n",
       "          ...   \n",
       "6627    0.107143\n",
       "6083    0.100000\n",
       "78      0.347826\n",
       "1063    0.320000\n",
       "3298    0.259259\n",
       "Name: 0, Length: 379, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "data = pd.read_csv(\"data.csv\",sep=\",\")\n",
    "Y = pd.read_csv(\"Y_1.csv\",sep=\",\")\n",
    "Y.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_epochs = 200 # 模型训练20轮\n",
    "log_interval = 210 #控制打印频率的，设n = 210*batch_size，即n张图后打印一次进度\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # 根据设备是否支持GPU来选择硬件 \n",
    "#DEVICE_1 = torch.device(\"cpu\") # 根据设备是否支持GPU来选择硬件 \n",
    "learn_rate = 0.03 # 学习率\n",
    "momentum = 0.1  # 动量\n",
    "network_fingerprints = fingerprints().to(DEVICE)\n",
    "network_fingerprints.eval()\n",
    "optimizer_fingerprints = optim.SGD(network_fingerprints.parameters(), lr=learn_rate, momentum=momentum) # 学习率，动量\n",
    "train_losses = []\n",
    "train_counter = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, device):\n",
    "    network_fingerprints.train() # 调用上一步实例化对象network中的方法（该方法包内已经写好）\n",
    "    for batch_idx, (data, target) in enumerate(trainloader_fp): # 按batch_size为集合对象进行逐个处理\n",
    "        data, target = data.to(device), target.to(device) # data是图片，target是标签，device是为了有GPU情况下使用GPU加速\n",
    "        optimizer_fingerprints.zero_grad() # 开始进行BP之前将梯度设置为零，因为PyTorch会在随后的BP中累积梯度    \n",
    "        output_fp = network_fingerprints(data) \n",
    "        loss_fn = nn.SmoothL1Loss()\n",
    "        loss2 = loss_fn(output_fp, target)\n",
    "        #print(loss)\n",
    "        loss2.backward(retain_graph=True) # 根据误差进行BP\n",
    "        optimizer_fingerprints.step()\n",
    "        if batch_idx % log_interval == 0: # 控制输出频率\n",
    "            print('Train fp Epoch: {} \\tLoss: {:.6f}'.format(epoch, loss2.item()))\n",
    "    #print(hi_fp.shape)\n",
    "    #print(hi_fv.shape)\n",
    "    #hi_fp = F.normalize(hi_fp,p=2,dim=0)\n",
    "    optimizer_fingerprints.step()\n",
    "    return loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error # 均方误差\n",
    "from sklearn.metrics import mean_absolute_error # 平方绝对误差\n",
    "def test(device,n):\n",
    "    network_fingerprints.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for data,target in testloader_fp:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = network_fingerprints(data)\n",
    "            loss_fn = nn.SmoothL1Loss()\n",
    "            out = output.cuda().data.cpu().detach().numpy()\n",
    "            tar = target.cuda().data.cpu().detach().numpy()\n",
    "            MSE = mean_squared_error(out, tar)\n",
    "            MAE = mean_absolute_error(out, tar)\n",
    "            print('test fp MSE: {:.6f}'.format(MSE))\n",
    "            print('test fp MAE: {:.6f}'.format(MAE))\n",
    "            #print(target)\n",
    "            #print(\"输出\")\n",
    "            loss = loss_fn(output, target)\n",
    "            i = i+1\n",
    "        #print(MSE)\n",
    "        #print(MAE)\n",
    "        #print(loss)\n",
    "        list = [MSE,MAE,loss.cuda().data.cpu().detach().numpy()]\n",
    "        df = pd.DataFrame(list)\n",
    "        df.to_csv(\"%d.csv\"%(n))\n",
    "        print('test fp Loss: {:.6f}'.format(loss.item()))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_file import read_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "paras = list(network_fingerprints.parameters())\n",
    "m1_1 = paras[0].clone()\n",
    "m2_2 = paras[1].clone()\n",
    "m3_3 = paras[2].clone()\n",
    "m4_4 = paras[3].clone()\n",
    "m5_5 = paras[4].clone()\n",
    "m6_6 = paras[5].clone()\n",
    "m7_7 = paras[6].clone()\n",
    "m8_8 = paras[7].clone()\n",
    "m9_9 = paras[8].clone()\n",
    "m10_10 = paras[9].clone()\n",
    "m11_11 = paras[10].clone()\n",
    "m12_12 = paras[11].clone()\n",
    "#m13_13 = paras[12].clone()\n",
    "X_train,X_test,y_train,y_test = train_test_split(data, Y.iloc[0],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = []\n",
    "list2 = []\n",
    "for i in range(0,len(y_train.index)):\n",
    "    #print(y_train.index[i])\n",
    "    #list.append(int(y_train.index[i]))\n",
    "    list1.append(y_train.index[i])\n",
    "for i in range(0,len(y_test.index)):\n",
    "    #print(y_train.index[i])\n",
    "    #list.append(int(y_train.index[i]))\n",
    "    list2.append(y_test.index[i])\n",
    "ytest=pd.DataFrame(list2)\n",
    "ytest.to_csv(\"ytest_tain.csv\")\n",
    "#list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个药方\n",
      "完成数据切分\n",
      "完成数据读取，开始稀疏化网络\n",
      "开始处理草药权重矩阵\n",
      "开始处理化合物权重矩阵\n",
      "\n",
      "开始处理基因权重矩阵\n",
      "\n",
      "开始处理通路权重矩阵\n",
      "\n",
      "开始处理功能权重矩阵\n",
      "开始训练和测试\n",
      "Train fp Epoch: 1 \tLoss: 0.069379\n",
      "Train fp Epoch: 2 \tLoss: 0.066734\n",
      "Train fp Epoch: 3 \tLoss: 0.064109\n",
      "Train fp Epoch: 4 \tLoss: 0.061652\n",
      "Train fp Epoch: 5 \tLoss: 0.059350\n",
      "Train fp Epoch: 6 \tLoss: 0.057190\n",
      "Train fp Epoch: 7 \tLoss: 0.055159\n",
      "Train fp Epoch: 8 \tLoss: 0.053246\n",
      "Train fp Epoch: 9 \tLoss: 0.051440\n",
      "Train fp Epoch: 10 \tLoss: 0.049733\n",
      "Train fp Epoch: 11 \tLoss: 0.048118\n",
      "Train fp Epoch: 12 \tLoss: 0.046588\n",
      "Train fp Epoch: 13 \tLoss: 0.045136\n",
      "Train fp Epoch: 14 \tLoss: 0.043758\n",
      "Train fp Epoch: 15 \tLoss: 0.042448\n",
      "Train fp Epoch: 16 \tLoss: 0.041203\n",
      "Train fp Epoch: 17 \tLoss: 0.040018\n",
      "Train fp Epoch: 18 \tLoss: 0.038890\n",
      "Train fp Epoch: 19 \tLoss: 0.037815\n",
      "Train fp Epoch: 20 \tLoss: 0.036788\n",
      "Train fp Epoch: 21 \tLoss: 0.035809\n",
      "Train fp Epoch: 22 \tLoss: 0.034872\n",
      "Train fp Epoch: 23 \tLoss: 0.033977\n",
      "Train fp Epoch: 24 \tLoss: 0.033122\n",
      "Train fp Epoch: 25 \tLoss: 0.032302\n",
      "Train fp Epoch: 26 \tLoss: 0.031518\n",
      "Train fp Epoch: 27 \tLoss: 0.030767\n",
      "Train fp Epoch: 28 \tLoss: 0.030047\n",
      "Train fp Epoch: 29 \tLoss: 0.029356\n",
      "Train fp Epoch: 30 \tLoss: 0.028694\n",
      "Train fp Epoch: 31 \tLoss: 0.028060\n",
      "Train fp Epoch: 32 \tLoss: 0.027451\n",
      "Train fp Epoch: 33 \tLoss: 0.026866\n",
      "Train fp Epoch: 34 \tLoss: 0.026305\n",
      "Train fp Epoch: 35 \tLoss: 0.025767\n",
      "Train fp Epoch: 36 \tLoss: 0.025249\n",
      "Train fp Epoch: 37 \tLoss: 0.024753\n",
      "Train fp Epoch: 38 \tLoss: 0.024276\n",
      "Train fp Epoch: 39 \tLoss: 0.023817\n",
      "Train fp Epoch: 40 \tLoss: 0.023377\n",
      "Train fp Epoch: 41 \tLoss: 0.022954\n",
      "Train fp Epoch: 42 \tLoss: 0.022547\n",
      "Train fp Epoch: 43 \tLoss: 0.022156\n",
      "Train fp Epoch: 44 \tLoss: 0.021780\n",
      "Train fp Epoch: 45 \tLoss: 0.021419\n",
      "Train fp Epoch: 46 \tLoss: 0.021072\n",
      "Train fp Epoch: 47 \tLoss: 0.020739\n",
      "Train fp Epoch: 48 \tLoss: 0.020418\n",
      "Train fp Epoch: 49 \tLoss: 0.020110\n",
      "Train fp Epoch: 50 \tLoss: 0.019814\n",
      "Train fp Epoch: 51 \tLoss: 0.019529\n",
      "Train fp Epoch: 52 \tLoss: 0.019256\n",
      "Train fp Epoch: 53 \tLoss: 0.018993\n",
      "Train fp Epoch: 54 \tLoss: 0.018740\n",
      "Train fp Epoch: 55 \tLoss: 0.018497\n",
      "Train fp Epoch: 56 \tLoss: 0.018264\n",
      "Train fp Epoch: 57 \tLoss: 0.018040\n",
      "Train fp Epoch: 58 \tLoss: 0.017825\n",
      "Train fp Epoch: 59 \tLoss: 0.017618\n",
      "Train fp Epoch: 60 \tLoss: 0.017419\n",
      "Train fp Epoch: 61 \tLoss: 0.017228\n",
      "Train fp Epoch: 62 \tLoss: 0.017045\n",
      "Train fp Epoch: 63 \tLoss: 0.016869\n",
      "Train fp Epoch: 64 \tLoss: 0.016700\n",
      "Train fp Epoch: 65 \tLoss: 0.016537\n",
      "Train fp Epoch: 66 \tLoss: 0.016381\n",
      "Train fp Epoch: 67 \tLoss: 0.016232\n",
      "Train fp Epoch: 68 \tLoss: 0.016088\n",
      "Train fp Epoch: 69 \tLoss: 0.015950\n",
      "Train fp Epoch: 70 \tLoss: 0.015818\n",
      "Train fp Epoch: 71 \tLoss: 0.015691\n",
      "Train fp Epoch: 72 \tLoss: 0.015569\n",
      "Train fp Epoch: 73 \tLoss: 0.015452\n",
      "Train fp Epoch: 74 \tLoss: 0.015340\n",
      "Train fp Epoch: 75 \tLoss: 0.015232\n",
      "Train fp Epoch: 76 \tLoss: 0.015129\n",
      "Train fp Epoch: 77 \tLoss: 0.015030\n",
      "Train fp Epoch: 78 \tLoss: 0.014935\n",
      "Train fp Epoch: 79 \tLoss: 0.014844\n",
      "Train fp Epoch: 80 \tLoss: 0.014756\n",
      "Train fp Epoch: 81 \tLoss: 0.014672\n",
      "Train fp Epoch: 82 \tLoss: 0.014592\n",
      "Train fp Epoch: 83 \tLoss: 0.014515\n",
      "Train fp Epoch: 84 \tLoss: 0.014441\n",
      "Train fp Epoch: 85 \tLoss: 0.014370\n",
      "Train fp Epoch: 86 \tLoss: 0.014302\n",
      "Train fp Epoch: 87 \tLoss: 0.014237\n",
      "Train fp Epoch: 88 \tLoss: 0.014175\n",
      "Train fp Epoch: 89 \tLoss: 0.014115\n",
      "Train fp Epoch: 90 \tLoss: 0.014057\n",
      "Train fp Epoch: 91 \tLoss: 0.014002\n",
      "Train fp Epoch: 92 \tLoss: 0.013950\n",
      "Train fp Epoch: 93 \tLoss: 0.013899\n",
      "Train fp Epoch: 94 \tLoss: 0.013851\n",
      "Train fp Epoch: 95 \tLoss: 0.013804\n",
      "Train fp Epoch: 96 \tLoss: 0.013760\n",
      "Train fp Epoch: 97 \tLoss: 0.013717\n",
      "Train fp Epoch: 98 \tLoss: 0.013676\n",
      "Train fp Epoch: 99 \tLoss: 0.013637\n",
      "Train fp Epoch: 100 \tLoss: 0.013599\n",
      "Train fp Epoch: 101 \tLoss: 0.013563\n",
      "Train fp Epoch: 102 \tLoss: 0.013529\n",
      "Train fp Epoch: 103 \tLoss: 0.013496\n",
      "Train fp Epoch: 104 \tLoss: 0.013464\n",
      "Train fp Epoch: 105 \tLoss: 0.013434\n",
      "Train fp Epoch: 106 \tLoss: 0.013404\n",
      "Train fp Epoch: 107 \tLoss: 0.013376\n",
      "Train fp Epoch: 108 \tLoss: 0.013350\n",
      "Train fp Epoch: 109 \tLoss: 0.013324\n",
      "Train fp Epoch: 110 \tLoss: 0.013299\n",
      "Train fp Epoch: 111 \tLoss: 0.013276\n",
      "Train fp Epoch: 112 \tLoss: 0.013253\n",
      "Train fp Epoch: 113 \tLoss: 0.013231\n",
      "Train fp Epoch: 114 \tLoss: 0.013210\n",
      "Train fp Epoch: 115 \tLoss: 0.013190\n",
      "Train fp Epoch: 116 \tLoss: 0.013171\n",
      "Train fp Epoch: 117 \tLoss: 0.013152\n",
      "Train fp Epoch: 118 \tLoss: 0.013135\n",
      "Train fp Epoch: 119 \tLoss: 0.013117\n",
      "Train fp Epoch: 120 \tLoss: 0.013101\n",
      "Train fp Epoch: 121 \tLoss: 0.013085\n",
      "Train fp Epoch: 122 \tLoss: 0.013070\n",
      "Train fp Epoch: 123 \tLoss: 0.013056\n",
      "Train fp Epoch: 124 \tLoss: 0.013042\n",
      "Train fp Epoch: 125 \tLoss: 0.013028\n",
      "Train fp Epoch: 126 \tLoss: 0.013015\n",
      "Train fp Epoch: 127 \tLoss: 0.013003\n",
      "Train fp Epoch: 128 \tLoss: 0.012991\n",
      "Train fp Epoch: 129 \tLoss: 0.012980\n",
      "Train fp Epoch: 130 \tLoss: 0.012969\n",
      "Train fp Epoch: 131 \tLoss: 0.012958\n",
      "Train fp Epoch: 132 \tLoss: 0.012948\n",
      "Train fp Epoch: 133 \tLoss: 0.012938\n",
      "Train fp Epoch: 134 \tLoss: 0.012928\n",
      "Train fp Epoch: 135 \tLoss: 0.012919\n",
      "Train fp Epoch: 136 \tLoss: 0.012910\n",
      "Train fp Epoch: 137 \tLoss: 0.012902\n",
      "Train fp Epoch: 138 \tLoss: 0.012894\n",
      "Train fp Epoch: 139 \tLoss: 0.012886\n",
      "Train fp Epoch: 140 \tLoss: 0.012878\n",
      "Train fp Epoch: 141 \tLoss: 0.012871\n",
      "Train fp Epoch: 142 \tLoss: 0.012864\n",
      "Train fp Epoch: 143 \tLoss: 0.012857\n",
      "Train fp Epoch: 144 \tLoss: 0.012850\n",
      "Train fp Epoch: 145 \tLoss: 0.012844\n",
      "Train fp Epoch: 146 \tLoss: 0.012837\n",
      "Train fp Epoch: 147 \tLoss: 0.012831\n",
      "Train fp Epoch: 148 \tLoss: 0.012826\n",
      "Train fp Epoch: 149 \tLoss: 0.012820\n",
      "Train fp Epoch: 150 \tLoss: 0.012815\n",
      "Train fp Epoch: 151 \tLoss: 0.012809\n",
      "Train fp Epoch: 152 \tLoss: 0.012804\n",
      "Train fp Epoch: 153 \tLoss: 0.012799\n",
      "Train fp Epoch: 154 \tLoss: 0.012794\n",
      "Train fp Epoch: 155 \tLoss: 0.012790\n",
      "Train fp Epoch: 156 \tLoss: 0.012785\n",
      "Train fp Epoch: 157 \tLoss: 0.012781\n",
      "Train fp Epoch: 158 \tLoss: 0.012777\n",
      "Train fp Epoch: 159 \tLoss: 0.012772\n",
      "Train fp Epoch: 160 \tLoss: 0.012768\n",
      "Train fp Epoch: 161 \tLoss: 0.012764\n",
      "Train fp Epoch: 162 \tLoss: 0.012761\n",
      "Train fp Epoch: 163 \tLoss: 0.012757\n",
      "Train fp Epoch: 164 \tLoss: 0.012753\n",
      "Train fp Epoch: 165 \tLoss: 0.012750\n",
      "Train fp Epoch: 166 \tLoss: 0.012746\n",
      "Train fp Epoch: 167 \tLoss: 0.012743\n",
      "Train fp Epoch: 168 \tLoss: 0.012740\n",
      "Train fp Epoch: 169 \tLoss: 0.012736\n",
      "Train fp Epoch: 170 \tLoss: 0.012733\n",
      "Train fp Epoch: 171 \tLoss: 0.012730\n",
      "Train fp Epoch: 172 \tLoss: 0.012727\n",
      "Train fp Epoch: 173 \tLoss: 0.012724\n",
      "Train fp Epoch: 174 \tLoss: 0.012721\n",
      "Train fp Epoch: 175 \tLoss: 0.012719\n",
      "Train fp Epoch: 176 \tLoss: 0.012716\n",
      "Train fp Epoch: 177 \tLoss: 0.012713\n",
      "Train fp Epoch: 178 \tLoss: 0.012711\n",
      "Train fp Epoch: 179 \tLoss: 0.012708\n",
      "Train fp Epoch: 180 \tLoss: 0.012705\n",
      "Train fp Epoch: 181 \tLoss: 0.012703\n",
      "Train fp Epoch: 182 \tLoss: 0.012701\n",
      "Train fp Epoch: 183 \tLoss: 0.012698\n",
      "Train fp Epoch: 184 \tLoss: 0.012696\n",
      "Train fp Epoch: 185 \tLoss: 0.012694\n",
      "Train fp Epoch: 186 \tLoss: 0.012691\n",
      "Train fp Epoch: 187 \tLoss: 0.012689\n",
      "Train fp Epoch: 188 \tLoss: 0.012687\n",
      "Train fp Epoch: 189 \tLoss: 0.012685\n",
      "Train fp Epoch: 190 \tLoss: 0.012683\n",
      "Train fp Epoch: 191 \tLoss: 0.012681\n",
      "Train fp Epoch: 192 \tLoss: 0.012679\n",
      "Train fp Epoch: 193 \tLoss: 0.012676\n",
      "Train fp Epoch: 194 \tLoss: 0.012675\n",
      "Train fp Epoch: 195 \tLoss: 0.012673\n",
      "Train fp Epoch: 196 \tLoss: 0.012671\n",
      "Train fp Epoch: 197 \tLoss: 0.012669\n",
      "Train fp Epoch: 198 \tLoss: 0.012667\n",
      "Train fp Epoch: 199 \tLoss: 0.012665\n",
      "Train fp Epoch: 200 \tLoss: 0.012663\n",
      "test fp MSE: 0.033145\n",
      "test fp MAE: 0.148344\n",
      "test fp Loss: 0.016290\n",
      "开始备份网络权重\n",
      "开始处理草药权重矩阵\n",
      "开始处理化合物权重矩阵\n",
      "\n",
      "开始处理基因权重矩阵\n",
      "\n",
      "开始处理通路权重矩阵\n",
      "\n",
      "开始处理功能权重矩阵\n",
      "第2个药方\n",
      "完成数据切分\n",
      "完成数据读取，开始稀疏化网络\n",
      "开始处理草药权重矩阵\n",
      "开始处理化合物权重矩阵\n",
      "\n",
      "开始处理基因权重矩阵\n",
      "\n",
      "开始处理通路权重矩阵\n",
      "\n",
      "开始处理功能权重矩阵\n",
      "开始训练和测试\n",
      "Train fp Epoch: 1 \tLoss: 0.014909\n",
      "Train fp Epoch: 2 \tLoss: 0.014905\n",
      "Train fp Epoch: 3 \tLoss: 0.014901\n",
      "Train fp Epoch: 4 \tLoss: 0.014897\n",
      "Train fp Epoch: 5 \tLoss: 0.014893\n",
      "Train fp Epoch: 6 \tLoss: 0.014889\n",
      "Train fp Epoch: 7 \tLoss: 0.014885\n",
      "Train fp Epoch: 8 \tLoss: 0.014881\n",
      "Train fp Epoch: 9 \tLoss: 0.014878\n",
      "Train fp Epoch: 10 \tLoss: 0.014875\n",
      "Train fp Epoch: 11 \tLoss: 0.014871\n",
      "Train fp Epoch: 12 \tLoss: 0.014868\n",
      "Train fp Epoch: 13 \tLoss: 0.014865\n",
      "Train fp Epoch: 14 \tLoss: 0.014862\n",
      "Train fp Epoch: 15 \tLoss: 0.014859\n",
      "Train fp Epoch: 16 \tLoss: 0.014856\n",
      "Train fp Epoch: 17 \tLoss: 0.014854\n",
      "Train fp Epoch: 18 \tLoss: 0.014851\n",
      "Train fp Epoch: 19 \tLoss: 0.014848\n",
      "Train fp Epoch: 20 \tLoss: 0.014846\n",
      "Train fp Epoch: 21 \tLoss: 0.014843\n",
      "Train fp Epoch: 22 \tLoss: 0.014841\n",
      "Train fp Epoch: 23 \tLoss: 0.014839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fp Epoch: 24 \tLoss: 0.014836\n",
      "Train fp Epoch: 25 \tLoss: 0.014834\n",
      "Train fp Epoch: 26 \tLoss: 0.014832\n",
      "Train fp Epoch: 27 \tLoss: 0.014830\n",
      "Train fp Epoch: 28 \tLoss: 0.014828\n",
      "Train fp Epoch: 29 \tLoss: 0.014826\n",
      "Train fp Epoch: 30 \tLoss: 0.014824\n",
      "Train fp Epoch: 31 \tLoss: 0.014822\n",
      "Train fp Epoch: 32 \tLoss: 0.014820\n",
      "Train fp Epoch: 33 \tLoss: 0.014819\n",
      "Train fp Epoch: 34 \tLoss: 0.014817\n",
      "Train fp Epoch: 35 \tLoss: 0.014815\n",
      "Train fp Epoch: 36 \tLoss: 0.014813\n",
      "Train fp Epoch: 37 \tLoss: 0.014812\n",
      "Train fp Epoch: 38 \tLoss: 0.014810\n",
      "Train fp Epoch: 39 \tLoss: 0.014809\n",
      "Train fp Epoch: 40 \tLoss: 0.014807\n",
      "Train fp Epoch: 41 \tLoss: 0.014806\n",
      "Train fp Epoch: 42 \tLoss: 0.014804\n",
      "Train fp Epoch: 43 \tLoss: 0.014803\n",
      "Train fp Epoch: 44 \tLoss: 0.014801\n",
      "Train fp Epoch: 45 \tLoss: 0.014800\n",
      "Train fp Epoch: 46 \tLoss: 0.014798\n",
      "Train fp Epoch: 47 \tLoss: 0.014797\n",
      "Train fp Epoch: 48 \tLoss: 0.014796\n",
      "Train fp Epoch: 49 \tLoss: 0.014794\n",
      "Train fp Epoch: 50 \tLoss: 0.014793\n",
      "Train fp Epoch: 51 \tLoss: 0.014792\n",
      "Train fp Epoch: 52 \tLoss: 0.014790\n",
      "Train fp Epoch: 53 \tLoss: 0.014789\n",
      "Train fp Epoch: 54 \tLoss: 0.014788\n",
      "Train fp Epoch: 55 \tLoss: 0.014787\n",
      "Train fp Epoch: 56 \tLoss: 0.014786\n",
      "Train fp Epoch: 57 \tLoss: 0.014784\n",
      "Train fp Epoch: 58 \tLoss: 0.014783\n",
      "Train fp Epoch: 59 \tLoss: 0.014782\n",
      "Train fp Epoch: 60 \tLoss: 0.014781\n",
      "Train fp Epoch: 61 \tLoss: 0.014780\n",
      "Train fp Epoch: 62 \tLoss: 0.014779\n",
      "Train fp Epoch: 63 \tLoss: 0.014778\n",
      "Train fp Epoch: 64 \tLoss: 0.014777\n",
      "Train fp Epoch: 65 \tLoss: 0.014776\n",
      "Train fp Epoch: 66 \tLoss: 0.014774\n",
      "Train fp Epoch: 67 \tLoss: 0.014773\n",
      "Train fp Epoch: 68 \tLoss: 0.014772\n",
      "Train fp Epoch: 69 \tLoss: 0.014771\n",
      "Train fp Epoch: 70 \tLoss: 0.014770\n",
      "Train fp Epoch: 71 \tLoss: 0.014769\n",
      "Train fp Epoch: 72 \tLoss: 0.014768\n",
      "Train fp Epoch: 73 \tLoss: 0.014767\n",
      "Train fp Epoch: 74 \tLoss: 0.014767\n",
      "Train fp Epoch: 75 \tLoss: 0.014766\n",
      "Train fp Epoch: 76 \tLoss: 0.014765\n",
      "Train fp Epoch: 77 \tLoss: 0.014764\n",
      "Train fp Epoch: 78 \tLoss: 0.014763\n",
      "Train fp Epoch: 79 \tLoss: 0.014762\n",
      "Train fp Epoch: 80 \tLoss: 0.014761\n",
      "Train fp Epoch: 81 \tLoss: 0.014760\n",
      "Train fp Epoch: 82 \tLoss: 0.014759\n",
      "Train fp Epoch: 83 \tLoss: 0.014758\n",
      "Train fp Epoch: 84 \tLoss: 0.014757\n",
      "Train fp Epoch: 85 \tLoss: 0.014756\n",
      "Train fp Epoch: 86 \tLoss: 0.014756\n",
      "Train fp Epoch: 87 \tLoss: 0.014755\n",
      "Train fp Epoch: 88 \tLoss: 0.014754\n",
      "Train fp Epoch: 89 \tLoss: 0.014753\n",
      "Train fp Epoch: 90 \tLoss: 0.014752\n",
      "Train fp Epoch: 91 \tLoss: 0.014751\n",
      "Train fp Epoch: 92 \tLoss: 0.014751\n",
      "Train fp Epoch: 93 \tLoss: 0.014750\n",
      "Train fp Epoch: 94 \tLoss: 0.014749\n",
      "Train fp Epoch: 95 \tLoss: 0.014748\n",
      "Train fp Epoch: 96 \tLoss: 0.014747\n",
      "Train fp Epoch: 97 \tLoss: 0.014747\n",
      "Train fp Epoch: 98 \tLoss: 0.014746\n",
      "Train fp Epoch: 99 \tLoss: 0.014745\n",
      "Train fp Epoch: 100 \tLoss: 0.014744\n",
      "Train fp Epoch: 101 \tLoss: 0.014743\n",
      "Train fp Epoch: 102 \tLoss: 0.014743\n",
      "Train fp Epoch: 103 \tLoss: 0.014742\n",
      "Train fp Epoch: 104 \tLoss: 0.014741\n",
      "Train fp Epoch: 105 \tLoss: 0.014740\n",
      "Train fp Epoch: 106 \tLoss: 0.014740\n",
      "Train fp Epoch: 107 \tLoss: 0.014739\n",
      "Train fp Epoch: 108 \tLoss: 0.014738\n",
      "Train fp Epoch: 109 \tLoss: 0.014737\n",
      "Train fp Epoch: 110 \tLoss: 0.014737\n",
      "Train fp Epoch: 111 \tLoss: 0.014736\n",
      "Train fp Epoch: 112 \tLoss: 0.014735\n",
      "Train fp Epoch: 113 \tLoss: 0.014735\n",
      "Train fp Epoch: 114 \tLoss: 0.014734\n",
      "Train fp Epoch: 115 \tLoss: 0.014733\n",
      "Train fp Epoch: 116 \tLoss: 0.014732\n",
      "Train fp Epoch: 117 \tLoss: 0.014732\n",
      "Train fp Epoch: 118 \tLoss: 0.014731\n",
      "Train fp Epoch: 119 \tLoss: 0.014730\n",
      "Train fp Epoch: 120 \tLoss: 0.014730\n",
      "Train fp Epoch: 121 \tLoss: 0.014729\n",
      "Train fp Epoch: 122 \tLoss: 0.014728\n",
      "Train fp Epoch: 123 \tLoss: 0.014728\n",
      "Train fp Epoch: 124 \tLoss: 0.014727\n",
      "Train fp Epoch: 125 \tLoss: 0.014726\n",
      "Train fp Epoch: 126 \tLoss: 0.014726\n",
      "Train fp Epoch: 127 \tLoss: 0.014725\n",
      "Train fp Epoch: 128 \tLoss: 0.014724\n",
      "Train fp Epoch: 129 \tLoss: 0.014724\n",
      "Train fp Epoch: 130 \tLoss: 0.014723\n",
      "Train fp Epoch: 131 \tLoss: 0.014722\n",
      "Train fp Epoch: 132 \tLoss: 0.014722\n",
      "Train fp Epoch: 133 \tLoss: 0.014721\n",
      "Train fp Epoch: 134 \tLoss: 0.014721\n",
      "Train fp Epoch: 135 \tLoss: 0.014720\n",
      "Train fp Epoch: 136 \tLoss: 0.014719\n",
      "Train fp Epoch: 137 \tLoss: 0.014719\n",
      "Train fp Epoch: 138 \tLoss: 0.014718\n",
      "Train fp Epoch: 139 \tLoss: 0.014718\n",
      "Train fp Epoch: 140 \tLoss: 0.014717\n",
      "Train fp Epoch: 141 \tLoss: 0.014716\n",
      "Train fp Epoch: 142 \tLoss: 0.014716\n",
      "Train fp Epoch: 143 \tLoss: 0.014715\n",
      "Train fp Epoch: 144 \tLoss: 0.014715\n",
      "Train fp Epoch: 145 \tLoss: 0.014714\n",
      "Train fp Epoch: 146 \tLoss: 0.014713\n",
      "Train fp Epoch: 147 \tLoss: 0.014713\n",
      "Train fp Epoch: 148 \tLoss: 0.014712\n",
      "Train fp Epoch: 149 \tLoss: 0.014712\n",
      "Train fp Epoch: 150 \tLoss: 0.014711\n",
      "Train fp Epoch: 151 \tLoss: 0.014710\n",
      "Train fp Epoch: 152 \tLoss: 0.014710\n",
      "Train fp Epoch: 153 \tLoss: 0.014709\n",
      "Train fp Epoch: 154 \tLoss: 0.014709\n",
      "Train fp Epoch: 155 \tLoss: 0.014708\n",
      "Train fp Epoch: 156 \tLoss: 0.014708\n",
      "Train fp Epoch: 157 \tLoss: 0.014707\n",
      "Train fp Epoch: 158 \tLoss: 0.014707\n",
      "Train fp Epoch: 159 \tLoss: 0.014706\n",
      "Train fp Epoch: 160 \tLoss: 0.014705\n",
      "Train fp Epoch: 161 \tLoss: 0.014705\n",
      "Train fp Epoch: 162 \tLoss: 0.014704\n",
      "Train fp Epoch: 163 \tLoss: 0.014704\n",
      "Train fp Epoch: 164 \tLoss: 0.014703\n",
      "Train fp Epoch: 165 \tLoss: 0.014703\n",
      "Train fp Epoch: 166 \tLoss: 0.014702\n",
      "Train fp Epoch: 167 \tLoss: 0.014702\n",
      "Train fp Epoch: 168 \tLoss: 0.014701\n",
      "Train fp Epoch: 169 \tLoss: 0.014701\n",
      "Train fp Epoch: 170 \tLoss: 0.014700\n",
      "Train fp Epoch: 171 \tLoss: 0.014700\n",
      "Train fp Epoch: 172 \tLoss: 0.014699\n",
      "Train fp Epoch: 173 \tLoss: 0.014699\n",
      "Train fp Epoch: 174 \tLoss: 0.014698\n",
      "Train fp Epoch: 175 \tLoss: 0.014698\n",
      "Train fp Epoch: 176 \tLoss: 0.014697\n",
      "Train fp Epoch: 177 \tLoss: 0.014697\n",
      "Train fp Epoch: 178 \tLoss: 0.014696\n",
      "Train fp Epoch: 179 \tLoss: 0.014696\n",
      "Train fp Epoch: 180 \tLoss: 0.014695\n",
      "Train fp Epoch: 181 \tLoss: 0.014695\n",
      "Train fp Epoch: 182 \tLoss: 0.014694\n",
      "Train fp Epoch: 183 \tLoss: 0.014694\n",
      "Train fp Epoch: 184 \tLoss: 0.014693\n",
      "Train fp Epoch: 185 \tLoss: 0.014693\n",
      "Train fp Epoch: 186 \tLoss: 0.014692\n",
      "Train fp Epoch: 187 \tLoss: 0.014692\n",
      "Train fp Epoch: 188 \tLoss: 0.014691\n",
      "Train fp Epoch: 189 \tLoss: 0.014691\n",
      "Train fp Epoch: 190 \tLoss: 0.014690\n",
      "Train fp Epoch: 191 \tLoss: 0.014690\n",
      "Train fp Epoch: 192 \tLoss: 0.014689\n",
      "Train fp Epoch: 193 \tLoss: 0.014689\n",
      "Train fp Epoch: 194 \tLoss: 0.014688\n",
      "Train fp Epoch: 195 \tLoss: 0.014688\n",
      "Train fp Epoch: 196 \tLoss: 0.014688\n",
      "Train fp Epoch: 197 \tLoss: 0.014687\n",
      "Train fp Epoch: 198 \tLoss: 0.014687\n",
      "Train fp Epoch: 199 \tLoss: 0.014686\n",
      "Train fp Epoch: 200 \tLoss: 0.014686\n",
      "test fp MSE: 0.042336\n",
      "test fp MAE: 0.165124\n",
      "test fp Loss: 0.020844\n",
      "开始备份网络权重\n",
      "开始处理草药权重矩阵\n",
      "开始处理化合物权重矩阵\n",
      "\n",
      "开始处理基因权重矩阵\n",
      "\n",
      "开始处理通路权重矩阵\n",
      "\n",
      "开始处理功能权重矩阵\n",
      "第3个药方\n",
      "完成数据切分\n",
      "完成数据读取，开始稀疏化网络\n",
      "开始处理草药权重矩阵\n",
      "开始处理化合物权重矩阵\n",
      "\n",
      "开始处理基因权重矩阵\n",
      "\n",
      "开始处理通路权重矩阵\n",
      "\n",
      "开始处理功能权重矩阵\n",
      "开始训练和测试\n",
      "Train fp Epoch: 1 \tLoss: 0.011331\n",
      "Train fp Epoch: 2 \tLoss: 0.011273\n",
      "Train fp Epoch: 3 \tLoss: 0.011213\n",
      "Train fp Epoch: 4 \tLoss: 0.011157\n",
      "Train fp Epoch: 5 \tLoss: 0.011102\n",
      "Train fp Epoch: 6 \tLoss: 0.011050\n",
      "Train fp Epoch: 7 \tLoss: 0.011000\n",
      "Train fp Epoch: 8 \tLoss: 0.010952\n",
      "Train fp Epoch: 9 \tLoss: 0.010906\n",
      "Train fp Epoch: 10 \tLoss: 0.010862\n",
      "Train fp Epoch: 11 \tLoss: 0.010820\n",
      "Train fp Epoch: 12 \tLoss: 0.010779\n",
      "Train fp Epoch: 13 \tLoss: 0.010741\n",
      "Train fp Epoch: 14 \tLoss: 0.010703\n",
      "Train fp Epoch: 15 \tLoss: 0.010668\n",
      "Train fp Epoch: 16 \tLoss: 0.010634\n",
      "Train fp Epoch: 17 \tLoss: 0.010601\n",
      "Train fp Epoch: 18 \tLoss: 0.010569\n",
      "Train fp Epoch: 19 \tLoss: 0.010539\n",
      "Train fp Epoch: 20 \tLoss: 0.010511\n",
      "Train fp Epoch: 21 \tLoss: 0.010483\n",
      "Train fp Epoch: 22 \tLoss: 0.010456\n",
      "Train fp Epoch: 23 \tLoss: 0.010431\n",
      "Train fp Epoch: 24 \tLoss: 0.010407\n",
      "Train fp Epoch: 25 \tLoss: 0.010383\n",
      "Train fp Epoch: 26 \tLoss: 0.010361\n",
      "Train fp Epoch: 27 \tLoss: 0.010340\n",
      "Train fp Epoch: 28 \tLoss: 0.010319\n",
      "Train fp Epoch: 29 \tLoss: 0.010300\n",
      "Train fp Epoch: 30 \tLoss: 0.010281\n",
      "Train fp Epoch: 31 \tLoss: 0.010263\n",
      "Train fp Epoch: 32 \tLoss: 0.010245\n",
      "Train fp Epoch: 33 \tLoss: 0.010229\n",
      "Train fp Epoch: 34 \tLoss: 0.010213\n",
      "Train fp Epoch: 35 \tLoss: 0.010198\n",
      "Train fp Epoch: 36 \tLoss: 0.010183\n",
      "Train fp Epoch: 37 \tLoss: 0.010169\n",
      "Train fp Epoch: 38 \tLoss: 0.010155\n",
      "Train fp Epoch: 39 \tLoss: 0.010142\n",
      "Train fp Epoch: 40 \tLoss: 0.010130\n",
      "Train fp Epoch: 41 \tLoss: 0.010118\n",
      "Train fp Epoch: 42 \tLoss: 0.010107\n",
      "Train fp Epoch: 43 \tLoss: 0.010096\n",
      "Train fp Epoch: 44 \tLoss: 0.010085\n",
      "Train fp Epoch: 45 \tLoss: 0.010075\n",
      "Train fp Epoch: 46 \tLoss: 0.010066\n",
      "Train fp Epoch: 47 \tLoss: 0.010056\n",
      "Train fp Epoch: 48 \tLoss: 0.010048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fp Epoch: 49 \tLoss: 0.010039\n",
      "Train fp Epoch: 50 \tLoss: 0.010031\n",
      "Train fp Epoch: 51 \tLoss: 0.010023\n",
      "Train fp Epoch: 52 \tLoss: 0.010015\n",
      "Train fp Epoch: 53 \tLoss: 0.010008\n",
      "Train fp Epoch: 54 \tLoss: 0.010001\n",
      "Train fp Epoch: 55 \tLoss: 0.009995\n",
      "Train fp Epoch: 56 \tLoss: 0.009988\n",
      "Train fp Epoch: 57 \tLoss: 0.009982\n",
      "Train fp Epoch: 58 \tLoss: 0.009976\n",
      "Train fp Epoch: 59 \tLoss: 0.009970\n",
      "Train fp Epoch: 60 \tLoss: 0.009965\n",
      "Train fp Epoch: 61 \tLoss: 0.009959\n",
      "Train fp Epoch: 62 \tLoss: 0.009954\n",
      "Train fp Epoch: 63 \tLoss: 0.009949\n",
      "Train fp Epoch: 64 \tLoss: 0.009945\n",
      "Train fp Epoch: 65 \tLoss: 0.009940\n",
      "Train fp Epoch: 66 \tLoss: 0.009936\n",
      "Train fp Epoch: 67 \tLoss: 0.009932\n",
      "Train fp Epoch: 68 \tLoss: 0.009928\n",
      "Train fp Epoch: 69 \tLoss: 0.009924\n",
      "Train fp Epoch: 70 \tLoss: 0.009920\n",
      "Train fp Epoch: 71 \tLoss: 0.009916\n",
      "Train fp Epoch: 72 \tLoss: 0.009913\n",
      "Train fp Epoch: 73 \tLoss: 0.009909\n",
      "Train fp Epoch: 74 \tLoss: 0.009906\n",
      "Train fp Epoch: 75 \tLoss: 0.009903\n",
      "Train fp Epoch: 76 \tLoss: 0.009900\n",
      "Train fp Epoch: 77 \tLoss: 0.009897\n",
      "Train fp Epoch: 78 \tLoss: 0.009894\n",
      "Train fp Epoch: 79 \tLoss: 0.009891\n",
      "Train fp Epoch: 80 \tLoss: 0.009889\n",
      "Train fp Epoch: 81 \tLoss: 0.009886\n",
      "Train fp Epoch: 82 \tLoss: 0.009883\n",
      "Train fp Epoch: 83 \tLoss: 0.009881\n",
      "Train fp Epoch: 84 \tLoss: 0.009879\n",
      "Train fp Epoch: 85 \tLoss: 0.009876\n",
      "Train fp Epoch: 86 \tLoss: 0.009874\n",
      "Train fp Epoch: 87 \tLoss: 0.009872\n",
      "Train fp Epoch: 88 \tLoss: 0.009870\n",
      "Train fp Epoch: 89 \tLoss: 0.009868\n",
      "Train fp Epoch: 90 \tLoss: 0.009866\n",
      "Train fp Epoch: 91 \tLoss: 0.009864\n",
      "Train fp Epoch: 92 \tLoss: 0.009862\n",
      "Train fp Epoch: 93 \tLoss: 0.009861\n",
      "Train fp Epoch: 94 \tLoss: 0.009859\n",
      "Train fp Epoch: 95 \tLoss: 0.009857\n",
      "Train fp Epoch: 96 \tLoss: 0.009855\n",
      "Train fp Epoch: 97 \tLoss: 0.009854\n",
      "Train fp Epoch: 98 \tLoss: 0.009852\n",
      "Train fp Epoch: 99 \tLoss: 0.009851\n",
      "Train fp Epoch: 100 \tLoss: 0.009849\n",
      "Train fp Epoch: 101 \tLoss: 0.009848\n",
      "Train fp Epoch: 102 \tLoss: 0.009846\n",
      "Train fp Epoch: 103 \tLoss: 0.009845\n",
      "Train fp Epoch: 104 \tLoss: 0.009844\n",
      "Train fp Epoch: 105 \tLoss: 0.009842\n",
      "Train fp Epoch: 106 \tLoss: 0.009841\n",
      "Train fp Epoch: 107 \tLoss: 0.009840\n",
      "Train fp Epoch: 108 \tLoss: 0.009839\n",
      "Train fp Epoch: 109 \tLoss: 0.009837\n",
      "Train fp Epoch: 110 \tLoss: 0.009836\n",
      "Train fp Epoch: 111 \tLoss: 0.009835\n",
      "Train fp Epoch: 112 \tLoss: 0.009834\n",
      "Train fp Epoch: 113 \tLoss: 0.009833\n",
      "Train fp Epoch: 114 \tLoss: 0.009832\n",
      "Train fp Epoch: 115 \tLoss: 0.009831\n",
      "Train fp Epoch: 116 \tLoss: 0.009830\n",
      "Train fp Epoch: 117 \tLoss: 0.009829\n",
      "Train fp Epoch: 118 \tLoss: 0.009828\n",
      "Train fp Epoch: 119 \tLoss: 0.009827\n",
      "Train fp Epoch: 120 \tLoss: 0.009826\n",
      "Train fp Epoch: 121 \tLoss: 0.009825\n",
      "Train fp Epoch: 122 \tLoss: 0.009824\n",
      "Train fp Epoch: 123 \tLoss: 0.009823\n",
      "Train fp Epoch: 124 \tLoss: 0.009822\n",
      "Train fp Epoch: 125 \tLoss: 0.009821\n",
      "Train fp Epoch: 126 \tLoss: 0.009820\n",
      "Train fp Epoch: 127 \tLoss: 0.009819\n",
      "Train fp Epoch: 128 \tLoss: 0.009818\n",
      "Train fp Epoch: 129 \tLoss: 0.009817\n",
      "Train fp Epoch: 130 \tLoss: 0.009817\n",
      "Train fp Epoch: 131 \tLoss: 0.009816\n",
      "Train fp Epoch: 132 \tLoss: 0.009815\n",
      "Train fp Epoch: 133 \tLoss: 0.009814\n",
      "Train fp Epoch: 134 \tLoss: 0.009813\n",
      "Train fp Epoch: 135 \tLoss: 0.009813\n",
      "Train fp Epoch: 136 \tLoss: 0.009812\n",
      "Train fp Epoch: 137 \tLoss: 0.009811\n",
      "Train fp Epoch: 138 \tLoss: 0.009810\n",
      "Train fp Epoch: 139 \tLoss: 0.009810\n",
      "Train fp Epoch: 140 \tLoss: 0.009809\n",
      "Train fp Epoch: 141 \tLoss: 0.009808\n",
      "Train fp Epoch: 142 \tLoss: 0.009807\n",
      "Train fp Epoch: 143 \tLoss: 0.009807\n",
      "Train fp Epoch: 144 \tLoss: 0.009806\n",
      "Train fp Epoch: 145 \tLoss: 0.009805\n",
      "Train fp Epoch: 146 \tLoss: 0.009805\n",
      "Train fp Epoch: 147 \tLoss: 0.009804\n",
      "Train fp Epoch: 148 \tLoss: 0.009803\n",
      "Train fp Epoch: 149 \tLoss: 0.009803\n",
      "Train fp Epoch: 150 \tLoss: 0.009802\n",
      "Train fp Epoch: 151 \tLoss: 0.009801\n",
      "Train fp Epoch: 152 \tLoss: 0.009801\n",
      "Train fp Epoch: 153 \tLoss: 0.009800\n",
      "Train fp Epoch: 154 \tLoss: 0.009799\n",
      "Train fp Epoch: 155 \tLoss: 0.009799\n",
      "Train fp Epoch: 156 \tLoss: 0.009798\n",
      "Train fp Epoch: 157 \tLoss: 0.009798\n",
      "Train fp Epoch: 158 \tLoss: 0.009797\n",
      "Train fp Epoch: 159 \tLoss: 0.009796\n",
      "Train fp Epoch: 160 \tLoss: 0.009796\n",
      "Train fp Epoch: 161 \tLoss: 0.009795\n",
      "Train fp Epoch: 162 \tLoss: 0.009795\n",
      "Train fp Epoch: 163 \tLoss: 0.009794\n",
      "Train fp Epoch: 164 \tLoss: 0.009793\n",
      "Train fp Epoch: 165 \tLoss: 0.009793\n",
      "Train fp Epoch: 166 \tLoss: 0.009792\n",
      "Train fp Epoch: 167 \tLoss: 0.009792\n",
      "Train fp Epoch: 168 \tLoss: 0.009791\n",
      "Train fp Epoch: 169 \tLoss: 0.009791\n",
      "Train fp Epoch: 170 \tLoss: 0.009790\n",
      "Train fp Epoch: 171 \tLoss: 0.009789\n",
      "Train fp Epoch: 172 \tLoss: 0.009789\n",
      "Train fp Epoch: 173 \tLoss: 0.009788\n",
      "Train fp Epoch: 174 \tLoss: 0.009788\n",
      "Train fp Epoch: 175 \tLoss: 0.009787\n",
      "Train fp Epoch: 176 \tLoss: 0.009787\n",
      "Train fp Epoch: 177 \tLoss: 0.009786\n",
      "Train fp Epoch: 178 \tLoss: 0.009786\n",
      "Train fp Epoch: 179 \tLoss: 0.009785\n",
      "Train fp Epoch: 180 \tLoss: 0.009785\n",
      "Train fp Epoch: 181 \tLoss: 0.009784\n",
      "Train fp Epoch: 182 \tLoss: 0.009784\n",
      "Train fp Epoch: 183 \tLoss: 0.009783\n",
      "Train fp Epoch: 184 \tLoss: 0.009783\n",
      "Train fp Epoch: 185 \tLoss: 0.009782\n",
      "Train fp Epoch: 186 \tLoss: 0.009782\n",
      "Train fp Epoch: 187 \tLoss: 0.009781\n",
      "Train fp Epoch: 188 \tLoss: 0.009781\n",
      "Train fp Epoch: 189 \tLoss: 0.009780\n",
      "Train fp Epoch: 190 \tLoss: 0.009780\n",
      "Train fp Epoch: 191 \tLoss: 0.009779\n",
      "Train fp Epoch: 192 \tLoss: 0.009779\n",
      "Train fp Epoch: 193 \tLoss: 0.009778\n",
      "Train fp Epoch: 194 \tLoss: 0.009778\n",
      "Train fp Epoch: 195 \tLoss: 0.009777\n",
      "Train fp Epoch: 196 \tLoss: 0.009777\n",
      "Train fp Epoch: 197 \tLoss: 0.009776\n",
      "Train fp Epoch: 198 \tLoss: 0.009776\n",
      "Train fp Epoch: 199 \tLoss: 0.009775\n",
      "Train fp Epoch: 200 \tLoss: 0.009775\n",
      "test fp MSE: 0.025945\n",
      "test fp MAE: 0.134051\n",
      "test fp Loss: 0.012482\n",
      "开始备份网络权重\n",
      "开始处理草药权重矩阵\n",
      "开始处理化合物权重矩阵\n",
      "\n",
      "开始处理基因权重矩阵\n",
      "\n",
      "开始处理通路权重矩阵\n",
      "\n",
      "开始处理功能权重矩阵\n",
      "第4个药方\n",
      "完成数据切分\n",
      "完成数据读取，开始稀疏化网络\n",
      "开始处理草药权重矩阵\n",
      "开始处理化合物权重矩阵\n",
      "\n",
      "开始处理基因权重矩阵\n",
      "\n",
      "开始处理通路权重矩阵\n",
      "\n",
      "开始处理功能权重矩阵\n",
      "开始训练和测试\n",
      "Train fp Epoch: 1 \tLoss: 0.045165\n",
      "Train fp Epoch: 2 \tLoss: 0.043476\n",
      "Train fp Epoch: 3 \tLoss: 0.041749\n",
      "Train fp Epoch: 4 \tLoss: 0.040084\n",
      "Train fp Epoch: 5 \tLoss: 0.038481\n",
      "Train fp Epoch: 6 \tLoss: 0.036938\n",
      "Train fp Epoch: 7 \tLoss: 0.035454\n",
      "Train fp Epoch: 8 \tLoss: 0.034029\n",
      "Train fp Epoch: 9 \tLoss: 0.032661\n",
      "Train fp Epoch: 10 \tLoss: 0.031349\n",
      "Train fp Epoch: 11 \tLoss: 0.030092\n",
      "Train fp Epoch: 12 \tLoss: 0.028888\n",
      "Train fp Epoch: 13 \tLoss: 0.027737\n",
      "Train fp Epoch: 14 \tLoss: 0.026637\n",
      "Train fp Epoch: 15 \tLoss: 0.025586\n",
      "Train fp Epoch: 16 \tLoss: 0.024584\n",
      "Train fp Epoch: 17 \tLoss: 0.023628\n",
      "Train fp Epoch: 18 \tLoss: 0.022717\n",
      "Train fp Epoch: 19 \tLoss: 0.021850\n",
      "Train fp Epoch: 20 \tLoss: 0.021025\n",
      "Train fp Epoch: 21 \tLoss: 0.020241\n",
      "Train fp Epoch: 22 \tLoss: 0.019495\n",
      "Train fp Epoch: 23 \tLoss: 0.018786\n",
      "Train fp Epoch: 24 \tLoss: 0.018112\n",
      "Train fp Epoch: 25 \tLoss: 0.017473\n",
      "Train fp Epoch: 26 \tLoss: 0.016865\n",
      "Train fp Epoch: 27 \tLoss: 0.016289\n",
      "Train fp Epoch: 28 \tLoss: 0.015742\n",
      "Train fp Epoch: 29 \tLoss: 0.015223\n",
      "Train fp Epoch: 30 \tLoss: 0.014730\n",
      "Train fp Epoch: 31 \tLoss: 0.014263\n",
      "Train fp Epoch: 32 \tLoss: 0.013819\n",
      "Train fp Epoch: 33 \tLoss: 0.013398\n",
      "Train fp Epoch: 34 \tLoss: 0.012999\n",
      "Train fp Epoch: 35 \tLoss: 0.012619\n",
      "Train fp Epoch: 36 \tLoss: 0.012259\n",
      "Train fp Epoch: 37 \tLoss: 0.011918\n",
      "Train fp Epoch: 38 \tLoss: 0.011593\n",
      "Train fp Epoch: 39 \tLoss: 0.011284\n",
      "Train fp Epoch: 40 \tLoss: 0.010991\n",
      "Train fp Epoch: 41 \tLoss: 0.010712\n",
      "Train fp Epoch: 42 \tLoss: 0.010447\n",
      "Train fp Epoch: 43 \tLoss: 0.010195\n",
      "Train fp Epoch: 44 \tLoss: 0.009955\n",
      "Train fp Epoch: 45 \tLoss: 0.009727\n",
      "Train fp Epoch: 46 \tLoss: 0.009510\n",
      "Train fp Epoch: 47 \tLoss: 0.009303\n",
      "Train fp Epoch: 48 \tLoss: 0.009105\n",
      "Train fp Epoch: 49 \tLoss: 0.008917\n",
      "Train fp Epoch: 50 \tLoss: 0.008738\n",
      "Train fp Epoch: 51 \tLoss: 0.008567\n",
      "Train fp Epoch: 52 \tLoss: 0.008403\n",
      "Train fp Epoch: 53 \tLoss: 0.008247\n",
      "Train fp Epoch: 54 \tLoss: 0.008098\n",
      "Train fp Epoch: 55 \tLoss: 0.007956\n",
      "Train fp Epoch: 56 \tLoss: 0.007820\n",
      "Train fp Epoch: 57 \tLoss: 0.007690\n",
      "Train fp Epoch: 58 \tLoss: 0.007565\n",
      "Train fp Epoch: 59 \tLoss: 0.007446\n",
      "Train fp Epoch: 60 \tLoss: 0.007332\n",
      "Train fp Epoch: 61 \tLoss: 0.007223\n",
      "Train fp Epoch: 62 \tLoss: 0.007118\n",
      "Train fp Epoch: 63 \tLoss: 0.007018\n",
      "Train fp Epoch: 64 \tLoss: 0.006922\n",
      "Train fp Epoch: 65 \tLoss: 0.006829\n",
      "Train fp Epoch: 66 \tLoss: 0.006741\n",
      "Train fp Epoch: 67 \tLoss: 0.006656\n",
      "Train fp Epoch: 68 \tLoss: 0.006574\n",
      "Train fp Epoch: 69 \tLoss: 0.006495\n",
      "Train fp Epoch: 70 \tLoss: 0.006420\n",
      "Train fp Epoch: 71 \tLoss: 0.006347\n",
      "Train fp Epoch: 72 \tLoss: 0.006277\n",
      "Train fp Epoch: 73 \tLoss: 0.006210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fp Epoch: 74 \tLoss: 0.006146\n",
      "Train fp Epoch: 75 \tLoss: 0.006083\n",
      "Train fp Epoch: 76 \tLoss: 0.006023\n",
      "Train fp Epoch: 77 \tLoss: 0.005966\n",
      "Train fp Epoch: 78 \tLoss: 0.005910\n",
      "Train fp Epoch: 79 \tLoss: 0.005856\n",
      "Train fp Epoch: 80 \tLoss: 0.005804\n",
      "Train fp Epoch: 81 \tLoss: 0.005755\n",
      "Train fp Epoch: 82 \tLoss: 0.005706\n",
      "Train fp Epoch: 83 \tLoss: 0.005660\n",
      "Train fp Epoch: 84 \tLoss: 0.005615\n",
      "Train fp Epoch: 85 \tLoss: 0.005571\n",
      "Train fp Epoch: 86 \tLoss: 0.005529\n",
      "Train fp Epoch: 87 \tLoss: 0.005489\n",
      "Train fp Epoch: 88 \tLoss: 0.005450\n",
      "Train fp Epoch: 89 \tLoss: 0.005412\n",
      "Train fp Epoch: 90 \tLoss: 0.005375\n",
      "Train fp Epoch: 91 \tLoss: 0.005340\n",
      "Train fp Epoch: 92 \tLoss: 0.005305\n",
      "Train fp Epoch: 93 \tLoss: 0.005272\n",
      "Train fp Epoch: 94 \tLoss: 0.005240\n",
      "Train fp Epoch: 95 \tLoss: 0.005208\n",
      "Train fp Epoch: 96 \tLoss: 0.005178\n",
      "Train fp Epoch: 97 \tLoss: 0.005149\n",
      "Train fp Epoch: 98 \tLoss: 0.005121\n",
      "Train fp Epoch: 99 \tLoss: 0.005093\n",
      "Train fp Epoch: 100 \tLoss: 0.005066\n",
      "Train fp Epoch: 101 \tLoss: 0.005040\n",
      "Train fp Epoch: 102 \tLoss: 0.005015\n",
      "Train fp Epoch: 103 \tLoss: 0.004991\n",
      "Train fp Epoch: 104 \tLoss: 0.004967\n",
      "Train fp Epoch: 105 \tLoss: 0.004944\n",
      "Train fp Epoch: 106 \tLoss: 0.004922\n",
      "Train fp Epoch: 107 \tLoss: 0.004900\n",
      "Train fp Epoch: 108 \tLoss: 0.004879\n",
      "Train fp Epoch: 109 \tLoss: 0.004858\n",
      "Train fp Epoch: 110 \tLoss: 0.004838\n",
      "Train fp Epoch: 111 \tLoss: 0.004819\n",
      "Train fp Epoch: 112 \tLoss: 0.004800\n",
      "Train fp Epoch: 113 \tLoss: 0.004782\n",
      "Train fp Epoch: 114 \tLoss: 0.004764\n",
      "Train fp Epoch: 115 \tLoss: 0.004747\n",
      "Train fp Epoch: 116 \tLoss: 0.004730\n",
      "Train fp Epoch: 117 \tLoss: 0.004713\n",
      "Train fp Epoch: 118 \tLoss: 0.004697\n",
      "Train fp Epoch: 119 \tLoss: 0.004682\n",
      "Train fp Epoch: 120 \tLoss: 0.004666\n",
      "Train fp Epoch: 121 \tLoss: 0.004652\n",
      "Train fp Epoch: 122 \tLoss: 0.004637\n",
      "Train fp Epoch: 123 \tLoss: 0.004623\n",
      "Train fp Epoch: 124 \tLoss: 0.004609\n",
      "Train fp Epoch: 125 \tLoss: 0.004596\n",
      "Train fp Epoch: 126 \tLoss: 0.004583\n",
      "Train fp Epoch: 127 \tLoss: 0.004570\n",
      "Train fp Epoch: 128 \tLoss: 0.004558\n",
      "Train fp Epoch: 129 \tLoss: 0.004546\n",
      "Train fp Epoch: 130 \tLoss: 0.004534\n",
      "Train fp Epoch: 131 \tLoss: 0.004522\n",
      "Train fp Epoch: 132 \tLoss: 0.004511\n",
      "Train fp Epoch: 133 \tLoss: 0.004500\n",
      "Train fp Epoch: 134 \tLoss: 0.004490\n",
      "Train fp Epoch: 135 \tLoss: 0.004479\n",
      "Train fp Epoch: 136 \tLoss: 0.004469\n",
      "Train fp Epoch: 137 \tLoss: 0.004459\n",
      "Train fp Epoch: 138 \tLoss: 0.004449\n",
      "Train fp Epoch: 139 \tLoss: 0.004440\n",
      "Train fp Epoch: 140 \tLoss: 0.004430\n",
      "Train fp Epoch: 141 \tLoss: 0.004421\n",
      "Train fp Epoch: 142 \tLoss: 0.004412\n",
      "Train fp Epoch: 143 \tLoss: 0.004404\n",
      "Train fp Epoch: 144 \tLoss: 0.004395\n",
      "Train fp Epoch: 145 \tLoss: 0.004387\n",
      "Train fp Epoch: 146 \tLoss: 0.004379\n",
      "Train fp Epoch: 147 \tLoss: 0.004371\n",
      "Train fp Epoch: 148 \tLoss: 0.004363\n",
      "Train fp Epoch: 149 \tLoss: 0.004355\n",
      "Train fp Epoch: 150 \tLoss: 0.004348\n",
      "Train fp Epoch: 151 \tLoss: 0.004341\n",
      "Train fp Epoch: 152 \tLoss: 0.004333\n",
      "Train fp Epoch: 153 \tLoss: 0.004326\n",
      "Train fp Epoch: 154 \tLoss: 0.004320\n",
      "Train fp Epoch: 155 \tLoss: 0.004313\n",
      "Train fp Epoch: 156 \tLoss: 0.004306\n",
      "Train fp Epoch: 157 \tLoss: 0.004300\n",
      "Train fp Epoch: 158 \tLoss: 0.004294\n",
      "Train fp Epoch: 159 \tLoss: 0.004287\n",
      "Train fp Epoch: 160 \tLoss: 0.004281\n",
      "Train fp Epoch: 161 \tLoss: 0.004275\n",
      "Train fp Epoch: 162 \tLoss: 0.004270\n",
      "Train fp Epoch: 163 \tLoss: 0.004264\n",
      "Train fp Epoch: 164 \tLoss: 0.004258\n",
      "Train fp Epoch: 165 \tLoss: 0.004253\n",
      "Train fp Epoch: 166 \tLoss: 0.004247\n",
      "Train fp Epoch: 167 \tLoss: 0.004242\n",
      "Train fp Epoch: 168 \tLoss: 0.004237\n",
      "Train fp Epoch: 169 \tLoss: 0.004232\n",
      "Train fp Epoch: 170 \tLoss: 0.004227\n",
      "Train fp Epoch: 171 \tLoss: 0.004222\n",
      "Train fp Epoch: 172 \tLoss: 0.004217\n",
      "Train fp Epoch: 173 \tLoss: 0.004213\n",
      "Train fp Epoch: 174 \tLoss: 0.004208\n",
      "Train fp Epoch: 175 \tLoss: 0.004204\n",
      "Train fp Epoch: 176 \tLoss: 0.004199\n",
      "Train fp Epoch: 177 \tLoss: 0.004195\n",
      "Train fp Epoch: 178 \tLoss: 0.004191\n",
      "Train fp Epoch: 179 \tLoss: 0.004186\n",
      "Train fp Epoch: 180 \tLoss: 0.004182\n",
      "Train fp Epoch: 181 \tLoss: 0.004178\n",
      "Train fp Epoch: 182 \tLoss: 0.004174\n",
      "Train fp Epoch: 183 \tLoss: 0.004170\n",
      "Train fp Epoch: 184 \tLoss: 0.004167\n",
      "Train fp Epoch: 185 \tLoss: 0.004163\n",
      "Train fp Epoch: 186 \tLoss: 0.004159\n",
      "Train fp Epoch: 187 \tLoss: 0.004155\n",
      "Train fp Epoch: 188 \tLoss: 0.004152\n",
      "Train fp Epoch: 189 \tLoss: 0.004148\n",
      "Train fp Epoch: 190 \tLoss: 0.004145\n",
      "Train fp Epoch: 191 \tLoss: 0.004142\n",
      "Train fp Epoch: 192 \tLoss: 0.004138\n",
      "Train fp Epoch: 193 \tLoss: 0.004135\n",
      "Train fp Epoch: 194 \tLoss: 0.004132\n",
      "Train fp Epoch: 195 \tLoss: 0.004129\n",
      "Train fp Epoch: 196 \tLoss: 0.004125\n",
      "Train fp Epoch: 197 \tLoss: 0.004122\n",
      "Train fp Epoch: 198 \tLoss: 0.004119\n",
      "Train fp Epoch: 199 \tLoss: 0.004116\n",
      "Train fp Epoch: 200 \tLoss: 0.004113\n",
      "test fp MSE: 0.005711\n",
      "test fp MAE: 0.048376\n",
      "test fp Loss: 0.003293\n",
      "开始备份网络权重\n",
      "开始处理草药权重矩阵\n",
      "开始处理化合物权重矩阵\n",
      "\n",
      "开始处理基因权重矩阵\n",
      "\n",
      "开始处理通路权重矩阵\n",
      "\n",
      "开始处理功能权重矩阵\n",
      "第5个药方\n",
      "完成数据切分\n",
      "完成数据读取，开始稀疏化网络\n",
      "开始处理草药权重矩阵\n",
      "开始处理化合物权重矩阵\n",
      "\n",
      "开始处理基因权重矩阵\n",
      "\n",
      "开始处理通路权重矩阵\n",
      "\n",
      "开始处理功能权重矩阵\n",
      "开始训练和测试\n",
      "Train fp Epoch: 1 \tLoss: 0.011588\n",
      "Train fp Epoch: 2 \tLoss: 0.011465\n",
      "Train fp Epoch: 3 \tLoss: 0.011333\n",
      "Train fp Epoch: 4 \tLoss: 0.011201\n",
      "Train fp Epoch: 5 \tLoss: 0.011069\n",
      "Train fp Epoch: 6 \tLoss: 0.010936\n",
      "Train fp Epoch: 7 \tLoss: 0.010804\n",
      "Train fp Epoch: 8 \tLoss: 0.010671\n",
      "Train fp Epoch: 9 \tLoss: 0.010538\n",
      "Train fp Epoch: 10 \tLoss: 0.010405\n",
      "Train fp Epoch: 11 \tLoss: 0.010273\n",
      "Train fp Epoch: 12 \tLoss: 0.010141\n",
      "Train fp Epoch: 13 \tLoss: 0.010009\n",
      "Train fp Epoch: 14 \tLoss: 0.009878\n",
      "Train fp Epoch: 15 \tLoss: 0.009747\n",
      "Train fp Epoch: 16 \tLoss: 0.009617\n",
      "Train fp Epoch: 17 \tLoss: 0.009487\n",
      "Train fp Epoch: 18 \tLoss: 0.009359\n",
      "Train fp Epoch: 19 \tLoss: 0.009231\n",
      "Train fp Epoch: 20 \tLoss: 0.009104\n",
      "Train fp Epoch: 21 \tLoss: 0.008978\n",
      "Train fp Epoch: 22 \tLoss: 0.008853\n",
      "Train fp Epoch: 23 \tLoss: 0.008730\n",
      "Train fp Epoch: 24 \tLoss: 0.008608\n",
      "Train fp Epoch: 25 \tLoss: 0.008487\n",
      "Train fp Epoch: 26 \tLoss: 0.008368\n",
      "Train fp Epoch: 27 \tLoss: 0.008250\n",
      "Train fp Epoch: 28 \tLoss: 0.008133\n",
      "Train fp Epoch: 29 \tLoss: 0.008019\n",
      "Train fp Epoch: 30 \tLoss: 0.007906\n",
      "Train fp Epoch: 31 \tLoss: 0.007794\n",
      "Train fp Epoch: 32 \tLoss: 0.007685\n",
      "Train fp Epoch: 33 \tLoss: 0.007577\n",
      "Train fp Epoch: 34 \tLoss: 0.007471\n",
      "Train fp Epoch: 35 \tLoss: 0.007367\n",
      "Train fp Epoch: 36 \tLoss: 0.007266\n",
      "Train fp Epoch: 37 \tLoss: 0.007166\n",
      "Train fp Epoch: 38 \tLoss: 0.007068\n",
      "Train fp Epoch: 39 \tLoss: 0.006972\n",
      "Train fp Epoch: 40 \tLoss: 0.006878\n",
      "Train fp Epoch: 41 \tLoss: 0.006786\n",
      "Train fp Epoch: 42 \tLoss: 0.006696\n",
      "Train fp Epoch: 43 \tLoss: 0.006608\n",
      "Train fp Epoch: 44 \tLoss: 0.006523\n",
      "Train fp Epoch: 45 \tLoss: 0.006439\n",
      "Train fp Epoch: 46 \tLoss: 0.006358\n",
      "Train fp Epoch: 47 \tLoss: 0.006278\n",
      "Train fp Epoch: 48 \tLoss: 0.006201\n",
      "Train fp Epoch: 49 \tLoss: 0.006126\n",
      "Train fp Epoch: 50 \tLoss: 0.006053\n",
      "Train fp Epoch: 51 \tLoss: 0.005982\n",
      "Train fp Epoch: 52 \tLoss: 0.005912\n",
      "Train fp Epoch: 53 \tLoss: 0.005845\n",
      "Train fp Epoch: 54 \tLoss: 0.005780\n",
      "Train fp Epoch: 55 \tLoss: 0.005717\n",
      "Train fp Epoch: 56 \tLoss: 0.005655\n",
      "Train fp Epoch: 57 \tLoss: 0.005596\n",
      "Train fp Epoch: 58 \tLoss: 0.005538\n",
      "Train fp Epoch: 59 \tLoss: 0.005482\n",
      "Train fp Epoch: 60 \tLoss: 0.005428\n",
      "Train fp Epoch: 61 \tLoss: 0.005375\n",
      "Train fp Epoch: 62 \tLoss: 0.005325\n",
      "Train fp Epoch: 63 \tLoss: 0.005276\n",
      "Train fp Epoch: 64 \tLoss: 0.005228\n",
      "Train fp Epoch: 65 \tLoss: 0.005183\n",
      "Train fp Epoch: 66 \tLoss: 0.005138\n",
      "Train fp Epoch: 67 \tLoss: 0.005096\n",
      "Train fp Epoch: 68 \tLoss: 0.005054\n",
      "Train fp Epoch: 69 \tLoss: 0.005015\n",
      "Train fp Epoch: 70 \tLoss: 0.004976\n",
      "Train fp Epoch: 71 \tLoss: 0.004939\n",
      "Train fp Epoch: 72 \tLoss: 0.004904\n",
      "Train fp Epoch: 73 \tLoss: 0.004869\n",
      "Train fp Epoch: 74 \tLoss: 0.004836\n",
      "Train fp Epoch: 75 \tLoss: 0.004804\n",
      "Train fp Epoch: 76 \tLoss: 0.004774\n",
      "Train fp Epoch: 77 \tLoss: 0.004744\n",
      "Train fp Epoch: 78 \tLoss: 0.004716\n",
      "Train fp Epoch: 79 \tLoss: 0.004688\n",
      "Train fp Epoch: 80 \tLoss: 0.004662\n",
      "Train fp Epoch: 81 \tLoss: 0.004636\n",
      "Train fp Epoch: 82 \tLoss: 0.004612\n",
      "Train fp Epoch: 83 \tLoss: 0.004589\n",
      "Train fp Epoch: 84 \tLoss: 0.004566\n",
      "Train fp Epoch: 85 \tLoss: 0.004544\n",
      "Train fp Epoch: 86 \tLoss: 0.004523\n",
      "Train fp Epoch: 87 \tLoss: 0.004503\n",
      "Train fp Epoch: 88 \tLoss: 0.004484\n",
      "Train fp Epoch: 89 \tLoss: 0.004466\n",
      "Train fp Epoch: 90 \tLoss: 0.004448\n",
      "Train fp Epoch: 91 \tLoss: 0.004431\n",
      "Train fp Epoch: 92 \tLoss: 0.004414\n",
      "Train fp Epoch: 93 \tLoss: 0.004399\n",
      "Train fp Epoch: 94 \tLoss: 0.004384\n",
      "Train fp Epoch: 95 \tLoss: 0.004369\n",
      "Train fp Epoch: 96 \tLoss: 0.004355\n",
      "Train fp Epoch: 97 \tLoss: 0.004342\n",
      "Train fp Epoch: 98 \tLoss: 0.004329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fp Epoch: 99 \tLoss: 0.004316\n",
      "Train fp Epoch: 100 \tLoss: 0.004305\n",
      "Train fp Epoch: 101 \tLoss: 0.004293\n",
      "Train fp Epoch: 102 \tLoss: 0.004282\n",
      "Train fp Epoch: 103 \tLoss: 0.004272\n",
      "Train fp Epoch: 104 \tLoss: 0.004262\n",
      "Train fp Epoch: 105 \tLoss: 0.004252\n",
      "Train fp Epoch: 106 \tLoss: 0.004243\n",
      "Train fp Epoch: 107 \tLoss: 0.004234\n",
      "Train fp Epoch: 108 \tLoss: 0.004225\n",
      "Train fp Epoch: 109 \tLoss: 0.004217\n",
      "Train fp Epoch: 110 \tLoss: 0.004209\n",
      "Train fp Epoch: 111 \tLoss: 0.004202\n",
      "Train fp Epoch: 112 \tLoss: 0.004194\n",
      "Train fp Epoch: 113 \tLoss: 0.004187\n",
      "Train fp Epoch: 114 \tLoss: 0.004181\n",
      "Train fp Epoch: 115 \tLoss: 0.004174\n",
      "Train fp Epoch: 116 \tLoss: 0.004168\n",
      "Train fp Epoch: 117 \tLoss: 0.004162\n",
      "Train fp Epoch: 118 \tLoss: 0.004157\n",
      "Train fp Epoch: 119 \tLoss: 0.004151\n",
      "Train fp Epoch: 120 \tLoss: 0.004146\n",
      "Train fp Epoch: 121 \tLoss: 0.004141\n",
      "Train fp Epoch: 122 \tLoss: 0.004136\n",
      "Train fp Epoch: 123 \tLoss: 0.004131\n",
      "Train fp Epoch: 124 \tLoss: 0.004127\n",
      "Train fp Epoch: 125 \tLoss: 0.004123\n",
      "Train fp Epoch: 126 \tLoss: 0.004118\n",
      "Train fp Epoch: 127 \tLoss: 0.004114\n",
      "Train fp Epoch: 128 \tLoss: 0.004111\n",
      "Train fp Epoch: 129 \tLoss: 0.004107\n",
      "Train fp Epoch: 130 \tLoss: 0.004103\n",
      "Train fp Epoch: 131 \tLoss: 0.004100\n",
      "Train fp Epoch: 132 \tLoss: 0.004097\n",
      "Train fp Epoch: 133 \tLoss: 0.004093\n",
      "Train fp Epoch: 134 \tLoss: 0.004090\n",
      "Train fp Epoch: 135 \tLoss: 0.004087\n",
      "Train fp Epoch: 136 \tLoss: 0.004085\n",
      "Train fp Epoch: 137 \tLoss: 0.004082\n",
      "Train fp Epoch: 138 \tLoss: 0.004079\n",
      "Train fp Epoch: 139 \tLoss: 0.004077\n",
      "Train fp Epoch: 140 \tLoss: 0.004074\n",
      "Train fp Epoch: 141 \tLoss: 0.004072\n",
      "Train fp Epoch: 142 \tLoss: 0.004070\n",
      "Train fp Epoch: 143 \tLoss: 0.004067\n",
      "Train fp Epoch: 144 \tLoss: 0.004065\n",
      "Train fp Epoch: 145 \tLoss: 0.004063\n",
      "Train fp Epoch: 146 \tLoss: 0.004061\n",
      "Train fp Epoch: 147 \tLoss: 0.004059\n",
      "Train fp Epoch: 148 \tLoss: 0.004058\n",
      "Train fp Epoch: 149 \tLoss: 0.004056\n",
      "Train fp Epoch: 150 \tLoss: 0.004054\n",
      "Train fp Epoch: 151 \tLoss: 0.004052\n",
      "Train fp Epoch: 152 \tLoss: 0.004051\n",
      "Train fp Epoch: 153 \tLoss: 0.004049\n",
      "Train fp Epoch: 154 \tLoss: 0.004048\n",
      "Train fp Epoch: 155 \tLoss: 0.004046\n",
      "Train fp Epoch: 156 \tLoss: 0.004045\n",
      "Train fp Epoch: 157 \tLoss: 0.004043\n",
      "Train fp Epoch: 158 \tLoss: 0.004042\n",
      "Train fp Epoch: 159 \tLoss: 0.004041\n",
      "Train fp Epoch: 160 \tLoss: 0.004039\n",
      "Train fp Epoch: 161 \tLoss: 0.004038\n",
      "Train fp Epoch: 162 \tLoss: 0.004037\n",
      "Train fp Epoch: 163 \tLoss: 0.004036\n",
      "Train fp Epoch: 164 \tLoss: 0.004035\n",
      "Train fp Epoch: 165 \tLoss: 0.004033\n",
      "Train fp Epoch: 166 \tLoss: 0.004032\n",
      "Train fp Epoch: 167 \tLoss: 0.004031\n",
      "Train fp Epoch: 168 \tLoss: 0.004030\n",
      "Train fp Epoch: 169 \tLoss: 0.004029\n",
      "Train fp Epoch: 170 \tLoss: 0.004028\n",
      "Train fp Epoch: 171 \tLoss: 0.004027\n",
      "Train fp Epoch: 172 \tLoss: 0.004026\n",
      "Train fp Epoch: 173 \tLoss: 0.004025\n",
      "Train fp Epoch: 174 \tLoss: 0.004025\n",
      "Train fp Epoch: 175 \tLoss: 0.004024\n",
      "Train fp Epoch: 176 \tLoss: 0.004023\n",
      "Train fp Epoch: 177 \tLoss: 0.004022\n",
      "Train fp Epoch: 178 \tLoss: 0.004021\n",
      "Train fp Epoch: 179 \tLoss: 0.004020\n",
      "Train fp Epoch: 180 \tLoss: 0.004020\n",
      "Train fp Epoch: 181 \tLoss: 0.004019\n",
      "Train fp Epoch: 182 \tLoss: 0.004018\n",
      "Train fp Epoch: 183 \tLoss: 0.004017\n",
      "Train fp Epoch: 184 \tLoss: 0.004017\n",
      "Train fp Epoch: 185 \tLoss: 0.004016\n",
      "Train fp Epoch: 186 \tLoss: 0.004015\n",
      "Train fp Epoch: 187 \tLoss: 0.004015\n",
      "Train fp Epoch: 188 \tLoss: 0.004014\n",
      "Train fp Epoch: 189 \tLoss: 0.004013\n",
      "Train fp Epoch: 190 \tLoss: 0.004013\n",
      "Train fp Epoch: 191 \tLoss: 0.004012\n",
      "Train fp Epoch: 192 \tLoss: 0.004011\n",
      "Train fp Epoch: 193 \tLoss: 0.004011\n",
      "Train fp Epoch: 194 \tLoss: 0.004010\n",
      "Train fp Epoch: 195 \tLoss: 0.004010\n",
      "Train fp Epoch: 196 \tLoss: 0.004009\n",
      "Train fp Epoch: 197 \tLoss: 0.004008\n",
      "Train fp Epoch: 198 \tLoss: 0.004008\n",
      "Train fp Epoch: 199 \tLoss: 0.004007\n",
      "Train fp Epoch: 200 \tLoss: 0.004007\n",
      "test fp MSE: 0.013498\n",
      "test fp MAE: 0.081441\n",
      "test fp Loss: 0.006259\n",
      "开始备份网络权重\n",
      "开始处理草药权重矩阵\n",
      "开始处理化合物权重矩阵\n",
      "\n",
      "开始处理基因权重矩阵\n",
      "\n",
      "开始处理通路权重矩阵\n",
      "\n",
      "开始处理功能权重矩阵\n",
      "第6个药方\n",
      "完成数据切分\n",
      "完成数据读取，开始稀疏化网络\n",
      "开始处理草药权重矩阵\n",
      "开始处理化合物权重矩阵\n",
      "\n",
      "开始处理基因权重矩阵\n",
      "\n",
      "开始处理通路权重矩阵\n",
      "\n",
      "开始处理功能权重矩阵\n",
      "开始训练和测试\n",
      "Train fp Epoch: 1 \tLoss: 0.004507\n",
      "Train fp Epoch: 2 \tLoss: 0.004501\n",
      "Train fp Epoch: 3 \tLoss: 0.004495\n",
      "Train fp Epoch: 4 \tLoss: 0.004489\n",
      "Train fp Epoch: 5 \tLoss: 0.004484\n",
      "Train fp Epoch: 6 \tLoss: 0.004478\n",
      "Train fp Epoch: 7 \tLoss: 0.004473\n",
      "Train fp Epoch: 8 \tLoss: 0.004468\n",
      "Train fp Epoch: 9 \tLoss: 0.004463\n",
      "Train fp Epoch: 10 \tLoss: 0.004459\n",
      "Train fp Epoch: 11 \tLoss: 0.004455\n",
      "Train fp Epoch: 12 \tLoss: 0.004451\n",
      "Train fp Epoch: 13 \tLoss: 0.004447\n",
      "Train fp Epoch: 14 \tLoss: 0.004443\n",
      "Train fp Epoch: 15 \tLoss: 0.004439\n",
      "Train fp Epoch: 16 \tLoss: 0.004436\n",
      "Train fp Epoch: 17 \tLoss: 0.004432\n",
      "Train fp Epoch: 18 \tLoss: 0.004429\n",
      "Train fp Epoch: 19 \tLoss: 0.004426\n",
      "Train fp Epoch: 20 \tLoss: 0.004423\n",
      "Train fp Epoch: 21 \tLoss: 0.004420\n",
      "Train fp Epoch: 22 \tLoss: 0.004418\n",
      "Train fp Epoch: 23 \tLoss: 0.004415\n",
      "Train fp Epoch: 24 \tLoss: 0.004413\n",
      "Train fp Epoch: 25 \tLoss: 0.004410\n",
      "Train fp Epoch: 26 \tLoss: 0.004408\n",
      "Train fp Epoch: 27 \tLoss: 0.004406\n",
      "Train fp Epoch: 28 \tLoss: 0.004404\n",
      "Train fp Epoch: 29 \tLoss: 0.004402\n",
      "Train fp Epoch: 30 \tLoss: 0.004400\n",
      "Train fp Epoch: 31 \tLoss: 0.004398\n",
      "Train fp Epoch: 32 \tLoss: 0.004396\n",
      "Train fp Epoch: 33 \tLoss: 0.004394\n",
      "Train fp Epoch: 34 \tLoss: 0.004393\n",
      "Train fp Epoch: 35 \tLoss: 0.004391\n",
      "Train fp Epoch: 36 \tLoss: 0.004390\n",
      "Train fp Epoch: 37 \tLoss: 0.004388\n",
      "Train fp Epoch: 38 \tLoss: 0.004387\n",
      "Train fp Epoch: 39 \tLoss: 0.004386\n",
      "Train fp Epoch: 40 \tLoss: 0.004384\n",
      "Train fp Epoch: 41 \tLoss: 0.004383\n",
      "Train fp Epoch: 42 \tLoss: 0.004382\n",
      "Train fp Epoch: 43 \tLoss: 0.004381\n",
      "Train fp Epoch: 44 \tLoss: 0.004380\n",
      "Train fp Epoch: 45 \tLoss: 0.004379\n",
      "Train fp Epoch: 46 \tLoss: 0.004378\n",
      "Train fp Epoch: 47 \tLoss: 0.004377\n",
      "Train fp Epoch: 48 \tLoss: 0.004376\n",
      "Train fp Epoch: 49 \tLoss: 0.004375\n",
      "Train fp Epoch: 50 \tLoss: 0.004374\n",
      "Train fp Epoch: 51 \tLoss: 0.004373\n",
      "Train fp Epoch: 52 \tLoss: 0.004372\n",
      "Train fp Epoch: 53 \tLoss: 0.004371\n",
      "Train fp Epoch: 54 \tLoss: 0.004371\n",
      "Train fp Epoch: 55 \tLoss: 0.004370\n",
      "Train fp Epoch: 56 \tLoss: 0.004369\n",
      "Train fp Epoch: 57 \tLoss: 0.004368\n",
      "Train fp Epoch: 58 \tLoss: 0.004368\n",
      "Train fp Epoch: 59 \tLoss: 0.004367\n",
      "Train fp Epoch: 60 \tLoss: 0.004366\n",
      "Train fp Epoch: 61 \tLoss: 0.004366\n",
      "Train fp Epoch: 62 \tLoss: 0.004365\n",
      "Train fp Epoch: 63 \tLoss: 0.004365\n",
      "Train fp Epoch: 64 \tLoss: 0.004364\n",
      "Train fp Epoch: 65 \tLoss: 0.004363\n",
      "Train fp Epoch: 66 \tLoss: 0.004363\n",
      "Train fp Epoch: 67 \tLoss: 0.004362\n",
      "Train fp Epoch: 68 \tLoss: 0.004362\n",
      "Train fp Epoch: 69 \tLoss: 0.004361\n",
      "Train fp Epoch: 70 \tLoss: 0.004361\n",
      "Train fp Epoch: 71 \tLoss: 0.004361\n",
      "Train fp Epoch: 72 \tLoss: 0.004360\n",
      "Train fp Epoch: 73 \tLoss: 0.004360\n",
      "Train fp Epoch: 74 \tLoss: 0.004359\n",
      "Train fp Epoch: 75 \tLoss: 0.004359\n",
      "Train fp Epoch: 76 \tLoss: 0.004358\n",
      "Train fp Epoch: 77 \tLoss: 0.004358\n",
      "Train fp Epoch: 78 \tLoss: 0.004358\n",
      "Train fp Epoch: 79 \tLoss: 0.004357\n",
      "Train fp Epoch: 80 \tLoss: 0.004357\n",
      "Train fp Epoch: 81 \tLoss: 0.004357\n",
      "Train fp Epoch: 82 \tLoss: 0.004356\n",
      "Train fp Epoch: 83 \tLoss: 0.004356\n",
      "Train fp Epoch: 84 \tLoss: 0.004355\n",
      "Train fp Epoch: 85 \tLoss: 0.004355\n",
      "Train fp Epoch: 86 \tLoss: 0.004355\n",
      "Train fp Epoch: 87 \tLoss: 0.004354\n",
      "Train fp Epoch: 88 \tLoss: 0.004354\n",
      "Train fp Epoch: 89 \tLoss: 0.004354\n",
      "Train fp Epoch: 90 \tLoss: 0.004354\n",
      "Train fp Epoch: 91 \tLoss: 0.004353\n",
      "Train fp Epoch: 92 \tLoss: 0.004353\n",
      "Train fp Epoch: 93 \tLoss: 0.004353\n",
      "Train fp Epoch: 94 \tLoss: 0.004352\n",
      "Train fp Epoch: 95 \tLoss: 0.004352\n",
      "Train fp Epoch: 96 \tLoss: 0.004352\n",
      "Train fp Epoch: 97 \tLoss: 0.004352\n",
      "Train fp Epoch: 98 \tLoss: 0.004351\n",
      "Train fp Epoch: 99 \tLoss: 0.004351\n",
      "Train fp Epoch: 100 \tLoss: 0.004351\n",
      "Train fp Epoch: 101 \tLoss: 0.004351\n",
      "Train fp Epoch: 102 \tLoss: 0.004350\n",
      "Train fp Epoch: 103 \tLoss: 0.004350\n",
      "Train fp Epoch: 104 \tLoss: 0.004350\n",
      "Train fp Epoch: 105 \tLoss: 0.004350\n",
      "Train fp Epoch: 106 \tLoss: 0.004349\n",
      "Train fp Epoch: 107 \tLoss: 0.004349\n",
      "Train fp Epoch: 108 \tLoss: 0.004349\n",
      "Train fp Epoch: 109 \tLoss: 0.004349\n",
      "Train fp Epoch: 110 \tLoss: 0.004348\n",
      "Train fp Epoch: 111 \tLoss: 0.004348\n",
      "Train fp Epoch: 112 \tLoss: 0.004348\n",
      "Train fp Epoch: 113 \tLoss: 0.004348\n",
      "Train fp Epoch: 114 \tLoss: 0.004347\n",
      "Train fp Epoch: 115 \tLoss: 0.004347\n",
      "Train fp Epoch: 116 \tLoss: 0.004347\n",
      "Train fp Epoch: 117 \tLoss: 0.004347\n",
      "Train fp Epoch: 118 \tLoss: 0.004347\n",
      "Train fp Epoch: 119 \tLoss: 0.004346\n",
      "Train fp Epoch: 120 \tLoss: 0.004346\n",
      "Train fp Epoch: 121 \tLoss: 0.004346\n",
      "Train fp Epoch: 122 \tLoss: 0.004346\n",
      "Train fp Epoch: 123 \tLoss: 0.004346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fp Epoch: 124 \tLoss: 0.004345\n",
      "Train fp Epoch: 125 \tLoss: 0.004345\n",
      "Train fp Epoch: 126 \tLoss: 0.004345\n",
      "Train fp Epoch: 127 \tLoss: 0.004345\n",
      "Train fp Epoch: 128 \tLoss: 0.004345\n",
      "Train fp Epoch: 129 \tLoss: 0.004344\n",
      "Train fp Epoch: 130 \tLoss: 0.004344\n",
      "Train fp Epoch: 131 \tLoss: 0.004344\n",
      "Train fp Epoch: 132 \tLoss: 0.004344\n",
      "Train fp Epoch: 133 \tLoss: 0.004344\n",
      "Train fp Epoch: 134 \tLoss: 0.004343\n",
      "Train fp Epoch: 135 \tLoss: 0.004343\n",
      "Train fp Epoch: 136 \tLoss: 0.004343\n",
      "Train fp Epoch: 137 \tLoss: 0.004343\n",
      "Train fp Epoch: 138 \tLoss: 0.004343\n",
      "Train fp Epoch: 139 \tLoss: 0.004343\n",
      "Train fp Epoch: 140 \tLoss: 0.004342\n",
      "Train fp Epoch: 141 \tLoss: 0.004342\n",
      "Train fp Epoch: 142 \tLoss: 0.004342\n",
      "Train fp Epoch: 143 \tLoss: 0.004342\n",
      "Train fp Epoch: 144 \tLoss: 0.004342\n",
      "Train fp Epoch: 145 \tLoss: 0.004341\n",
      "Train fp Epoch: 146 \tLoss: 0.004341\n",
      "Train fp Epoch: 147 \tLoss: 0.004341\n",
      "Train fp Epoch: 148 \tLoss: 0.004341\n",
      "Train fp Epoch: 149 \tLoss: 0.004341\n",
      "Train fp Epoch: 150 \tLoss: 0.004341\n",
      "Train fp Epoch: 151 \tLoss: 0.004340\n",
      "Train fp Epoch: 152 \tLoss: 0.004340\n",
      "Train fp Epoch: 153 \tLoss: 0.004340\n",
      "Train fp Epoch: 154 \tLoss: 0.004340\n",
      "Train fp Epoch: 155 \tLoss: 0.004340\n",
      "Train fp Epoch: 156 \tLoss: 0.004340\n",
      "Train fp Epoch: 157 \tLoss: 0.004339\n",
      "Train fp Epoch: 158 \tLoss: 0.004339\n",
      "Train fp Epoch: 159 \tLoss: 0.004339\n",
      "Train fp Epoch: 160 \tLoss: 0.004339\n",
      "Train fp Epoch: 161 \tLoss: 0.004339\n",
      "Train fp Epoch: 162 \tLoss: 0.004339\n",
      "Train fp Epoch: 163 \tLoss: 0.004338\n",
      "Train fp Epoch: 164 \tLoss: 0.004338\n",
      "Train fp Epoch: 165 \tLoss: 0.004338\n",
      "Train fp Epoch: 166 \tLoss: 0.004338\n",
      "Train fp Epoch: 167 \tLoss: 0.004338\n",
      "Train fp Epoch: 168 \tLoss: 0.004338\n",
      "Train fp Epoch: 169 \tLoss: 0.004338\n",
      "Train fp Epoch: 170 \tLoss: 0.004337\n",
      "Train fp Epoch: 171 \tLoss: 0.004337\n",
      "Train fp Epoch: 172 \tLoss: 0.004337\n",
      "Train fp Epoch: 173 \tLoss: 0.004337\n",
      "Train fp Epoch: 174 \tLoss: 0.004337\n",
      "Train fp Epoch: 175 \tLoss: 0.004337\n",
      "Train fp Epoch: 176 \tLoss: 0.004336\n",
      "Train fp Epoch: 177 \tLoss: 0.004336\n",
      "Train fp Epoch: 178 \tLoss: 0.004336\n",
      "Train fp Epoch: 179 \tLoss: 0.004336\n",
      "Train fp Epoch: 180 \tLoss: 0.004336\n",
      "Train fp Epoch: 181 \tLoss: 0.004336\n",
      "Train fp Epoch: 182 \tLoss: 0.004336\n",
      "Train fp Epoch: 183 \tLoss: 0.004335\n",
      "Train fp Epoch: 184 \tLoss: 0.004335\n",
      "Train fp Epoch: 185 \tLoss: 0.004335\n",
      "Train fp Epoch: 186 \tLoss: 0.004335\n",
      "Train fp Epoch: 187 \tLoss: 0.004335\n",
      "Train fp Epoch: 188 \tLoss: 0.004335\n",
      "Train fp Epoch: 189 \tLoss: 0.004334\n",
      "Train fp Epoch: 190 \tLoss: 0.004334\n",
      "Train fp Epoch: 191 \tLoss: 0.004334\n",
      "Train fp Epoch: 192 \tLoss: 0.004334\n",
      "Train fp Epoch: 193 \tLoss: 0.004334\n",
      "Train fp Epoch: 194 \tLoss: 0.004334\n",
      "Train fp Epoch: 195 \tLoss: 0.004334\n",
      "Train fp Epoch: 196 \tLoss: 0.004333\n",
      "Train fp Epoch: 197 \tLoss: 0.004333\n",
      "Train fp Epoch: 198 \tLoss: 0.004333\n",
      "Train fp Epoch: 199 \tLoss: 0.004333\n",
      "Train fp Epoch: 200 \tLoss: 0.004333\n",
      "test fp MSE: 0.012575\n",
      "test fp MAE: 0.079718\n",
      "test fp Loss: 0.005959\n",
      "开始备份网络权重\n",
      "开始处理草药权重矩阵\n",
      "开始处理化合物权重矩阵\n",
      "\n",
      "开始处理基因权重矩阵\n",
      "\n",
      "开始处理通路权重矩阵\n",
      "\n",
      "开始处理功能权重矩阵\n",
      "第7个药方\n",
      "完成数据切分\n",
      "完成数据读取，开始稀疏化网络\n",
      "开始处理草药权重矩阵\n",
      "开始处理化合物权重矩阵\n",
      "\n",
      "开始处理基因权重矩阵\n",
      "\n",
      "开始处理通路权重矩阵\n",
      "\n",
      "开始处理功能权重矩阵\n",
      "开始训练和测试\n",
      "Train fp Epoch: 1 \tLoss: 0.020184\n",
      "Train fp Epoch: 2 \tLoss: 0.019797\n",
      "Train fp Epoch: 3 \tLoss: 0.019406\n",
      "Train fp Epoch: 4 \tLoss: 0.019031\n",
      "Train fp Epoch: 5 \tLoss: 0.018674\n",
      "Train fp Epoch: 6 \tLoss: 0.018332\n",
      "Train fp Epoch: 7 \tLoss: 0.018006\n",
      "Train fp Epoch: 8 \tLoss: 0.017695\n",
      "Train fp Epoch: 9 \tLoss: 0.017398\n",
      "Train fp Epoch: 10 \tLoss: 0.017114\n",
      "Train fp Epoch: 11 \tLoss: 0.016844\n",
      "Train fp Epoch: 12 \tLoss: 0.016587\n",
      "Train fp Epoch: 13 \tLoss: 0.016341\n",
      "Train fp Epoch: 14 \tLoss: 0.016107\n",
      "Train fp Epoch: 15 \tLoss: 0.015884\n",
      "Train fp Epoch: 16 \tLoss: 0.015672\n",
      "Train fp Epoch: 17 \tLoss: 0.015469\n",
      "Train fp Epoch: 18 \tLoss: 0.015276\n",
      "Train fp Epoch: 19 \tLoss: 0.015092\n",
      "Train fp Epoch: 20 \tLoss: 0.014917\n",
      "Train fp Epoch: 21 \tLoss: 0.014749\n",
      "Train fp Epoch: 22 \tLoss: 0.014590\n",
      "Train fp Epoch: 23 \tLoss: 0.014438\n",
      "Train fp Epoch: 24 \tLoss: 0.014293\n",
      "Train fp Epoch: 25 \tLoss: 0.014154\n",
      "Train fp Epoch: 26 \tLoss: 0.014022\n",
      "Train fp Epoch: 27 \tLoss: 0.013897\n",
      "Train fp Epoch: 28 \tLoss: 0.013777\n",
      "Train fp Epoch: 29 \tLoss: 0.013662\n",
      "Train fp Epoch: 30 \tLoss: 0.013553\n",
      "Train fp Epoch: 31 \tLoss: 0.013449\n",
      "Train fp Epoch: 32 \tLoss: 0.013349\n",
      "Train fp Epoch: 33 \tLoss: 0.013254\n",
      "Train fp Epoch: 34 \tLoss: 0.013164\n",
      "Train fp Epoch: 35 \tLoss: 0.013077\n",
      "Train fp Epoch: 36 \tLoss: 0.012995\n",
      "Train fp Epoch: 37 \tLoss: 0.012916\n",
      "Train fp Epoch: 38 \tLoss: 0.012840\n",
      "Train fp Epoch: 39 \tLoss: 0.012769\n",
      "Train fp Epoch: 40 \tLoss: 0.012700\n",
      "Train fp Epoch: 41 \tLoss: 0.012634\n",
      "Train fp Epoch: 42 \tLoss: 0.012572\n",
      "Train fp Epoch: 43 \tLoss: 0.012512\n",
      "Train fp Epoch: 44 \tLoss: 0.012454\n",
      "Train fp Epoch: 45 \tLoss: 0.012400\n",
      "Train fp Epoch: 46 \tLoss: 0.012347\n",
      "Train fp Epoch: 47 \tLoss: 0.012297\n",
      "Train fp Epoch: 48 \tLoss: 0.012250\n",
      "Train fp Epoch: 49 \tLoss: 0.012204\n",
      "Train fp Epoch: 50 \tLoss: 0.012160\n",
      "Train fp Epoch: 51 \tLoss: 0.012118\n",
      "Train fp Epoch: 52 \tLoss: 0.012078\n",
      "Train fp Epoch: 53 \tLoss: 0.012040\n",
      "Train fp Epoch: 54 \tLoss: 0.012004\n",
      "Train fp Epoch: 55 \tLoss: 0.011968\n",
      "Train fp Epoch: 56 \tLoss: 0.011935\n",
      "Train fp Epoch: 57 \tLoss: 0.011903\n",
      "Train fp Epoch: 58 \tLoss: 0.011872\n",
      "Train fp Epoch: 59 \tLoss: 0.011843\n",
      "Train fp Epoch: 60 \tLoss: 0.011815\n",
      "Train fp Epoch: 61 \tLoss: 0.011788\n",
      "Train fp Epoch: 62 \tLoss: 0.011762\n",
      "Train fp Epoch: 63 \tLoss: 0.011737\n",
      "Train fp Epoch: 64 \tLoss: 0.011714\n",
      "Train fp Epoch: 65 \tLoss: 0.011691\n",
      "Train fp Epoch: 66 \tLoss: 0.011669\n",
      "Train fp Epoch: 67 \tLoss: 0.011649\n",
      "Train fp Epoch: 68 \tLoss: 0.011629\n",
      "Train fp Epoch: 69 \tLoss: 0.011610\n",
      "Train fp Epoch: 70 \tLoss: 0.011591\n",
      "Train fp Epoch: 71 \tLoss: 0.011574\n",
      "Train fp Epoch: 72 \tLoss: 0.011557\n",
      "Train fp Epoch: 73 \tLoss: 0.011541\n",
      "Train fp Epoch: 74 \tLoss: 0.011526\n",
      "Train fp Epoch: 75 \tLoss: 0.011511\n",
      "Train fp Epoch: 76 \tLoss: 0.011497\n",
      "Train fp Epoch: 77 \tLoss: 0.011483\n",
      "Train fp Epoch: 78 \tLoss: 0.011470\n",
      "Train fp Epoch: 79 \tLoss: 0.011458\n",
      "Train fp Epoch: 80 \tLoss: 0.011446\n",
      "Train fp Epoch: 81 \tLoss: 0.011434\n",
      "Train fp Epoch: 82 \tLoss: 0.011423\n",
      "Train fp Epoch: 83 \tLoss: 0.011413\n",
      "Train fp Epoch: 84 \tLoss: 0.011403\n",
      "Train fp Epoch: 85 \tLoss: 0.011393\n",
      "Train fp Epoch: 86 \tLoss: 0.011384\n",
      "Train fp Epoch: 87 \tLoss: 0.011375\n",
      "Train fp Epoch: 88 \tLoss: 0.011366\n",
      "Train fp Epoch: 89 \tLoss: 0.011358\n",
      "Train fp Epoch: 90 \tLoss: 0.011350\n",
      "Train fp Epoch: 91 \tLoss: 0.011343\n",
      "Train fp Epoch: 92 \tLoss: 0.011335\n",
      "Train fp Epoch: 93 \tLoss: 0.011329\n",
      "Train fp Epoch: 94 \tLoss: 0.011322\n",
      "Train fp Epoch: 95 \tLoss: 0.011315\n",
      "Train fp Epoch: 96 \tLoss: 0.011309\n",
      "Train fp Epoch: 97 \tLoss: 0.011303\n",
      "Train fp Epoch: 98 \tLoss: 0.011298\n",
      "Train fp Epoch: 99 \tLoss: 0.011292\n",
      "Train fp Epoch: 100 \tLoss: 0.011287\n",
      "Train fp Epoch: 101 \tLoss: 0.011282\n",
      "Train fp Epoch: 102 \tLoss: 0.011277\n",
      "Train fp Epoch: 103 \tLoss: 0.011273\n",
      "Train fp Epoch: 104 \tLoss: 0.011268\n",
      "Train fp Epoch: 105 \tLoss: 0.011264\n",
      "Train fp Epoch: 106 \tLoss: 0.011260\n",
      "Train fp Epoch: 107 \tLoss: 0.011256\n",
      "Train fp Epoch: 108 \tLoss: 0.011252\n",
      "Train fp Epoch: 109 \tLoss: 0.011249\n",
      "Train fp Epoch: 110 \tLoss: 0.011245\n",
      "Train fp Epoch: 111 \tLoss: 0.011242\n",
      "Train fp Epoch: 112 \tLoss: 0.011239\n",
      "Train fp Epoch: 113 \tLoss: 0.011235\n",
      "Train fp Epoch: 114 \tLoss: 0.011233\n",
      "Train fp Epoch: 115 \tLoss: 0.011230\n",
      "Train fp Epoch: 116 \tLoss: 0.011227\n",
      "Train fp Epoch: 117 \tLoss: 0.011224\n",
      "Train fp Epoch: 118 \tLoss: 0.011222\n",
      "Train fp Epoch: 119 \tLoss: 0.011219\n",
      "Train fp Epoch: 120 \tLoss: 0.011217\n",
      "Train fp Epoch: 121 \tLoss: 0.011215\n",
      "Train fp Epoch: 122 \tLoss: 0.011213\n",
      "Train fp Epoch: 123 \tLoss: 0.011211\n",
      "Train fp Epoch: 124 \tLoss: 0.011209\n",
      "Train fp Epoch: 125 \tLoss: 0.011207\n",
      "Train fp Epoch: 126 \tLoss: 0.011205\n",
      "Train fp Epoch: 127 \tLoss: 0.011203\n",
      "Train fp Epoch: 128 \tLoss: 0.011201\n",
      "Train fp Epoch: 129 \tLoss: 0.011200\n",
      "Train fp Epoch: 130 \tLoss: 0.011198\n",
      "Train fp Epoch: 131 \tLoss: 0.011197\n",
      "Train fp Epoch: 132 \tLoss: 0.011195\n",
      "Train fp Epoch: 133 \tLoss: 0.011194\n",
      "Train fp Epoch: 134 \tLoss: 0.011192\n",
      "Train fp Epoch: 135 \tLoss: 0.011191\n",
      "Train fp Epoch: 136 \tLoss: 0.011190\n",
      "Train fp Epoch: 137 \tLoss: 0.011189\n",
      "Train fp Epoch: 138 \tLoss: 0.011187\n",
      "Train fp Epoch: 139 \tLoss: 0.011186\n",
      "Train fp Epoch: 140 \tLoss: 0.011185\n",
      "Train fp Epoch: 141 \tLoss: 0.011184\n",
      "Train fp Epoch: 142 \tLoss: 0.011183\n",
      "Train fp Epoch: 143 \tLoss: 0.011182\n",
      "Train fp Epoch: 144 \tLoss: 0.011181\n",
      "Train fp Epoch: 145 \tLoss: 0.011180\n",
      "Train fp Epoch: 146 \tLoss: 0.011179\n",
      "Train fp Epoch: 147 \tLoss: 0.011179\n",
      "Train fp Epoch: 148 \tLoss: 0.011178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fp Epoch: 149 \tLoss: 0.011177\n",
      "Train fp Epoch: 150 \tLoss: 0.011176\n",
      "Train fp Epoch: 151 \tLoss: 0.011176\n",
      "Train fp Epoch: 152 \tLoss: 0.011175\n",
      "Train fp Epoch: 153 \tLoss: 0.011174\n",
      "Train fp Epoch: 154 \tLoss: 0.011174\n",
      "Train fp Epoch: 155 \tLoss: 0.011173\n",
      "Train fp Epoch: 156 \tLoss: 0.011172\n",
      "Train fp Epoch: 157 \tLoss: 0.011172\n",
      "Train fp Epoch: 158 \tLoss: 0.011171\n",
      "Train fp Epoch: 159 \tLoss: 0.011171\n",
      "Train fp Epoch: 160 \tLoss: 0.011170\n",
      "Train fp Epoch: 161 \tLoss: 0.011170\n",
      "Train fp Epoch: 162 \tLoss: 0.011169\n",
      "Train fp Epoch: 163 \tLoss: 0.011169\n",
      "Train fp Epoch: 164 \tLoss: 0.011168\n",
      "Train fp Epoch: 165 \tLoss: 0.011168\n",
      "Train fp Epoch: 166 \tLoss: 0.011167\n",
      "Train fp Epoch: 167 \tLoss: 0.011167\n",
      "Train fp Epoch: 168 \tLoss: 0.011167\n",
      "Train fp Epoch: 169 \tLoss: 0.011166\n",
      "Train fp Epoch: 170 \tLoss: 0.011166\n",
      "Train fp Epoch: 171 \tLoss: 0.011165\n",
      "Train fp Epoch: 172 \tLoss: 0.011165\n",
      "Train fp Epoch: 173 \tLoss: 0.011165\n",
      "Train fp Epoch: 174 \tLoss: 0.011164\n",
      "Train fp Epoch: 175 \tLoss: 0.011164\n",
      "Train fp Epoch: 176 \tLoss: 0.011164\n",
      "Train fp Epoch: 177 \tLoss: 0.011163\n",
      "Train fp Epoch: 178 \tLoss: 0.011163\n",
      "Train fp Epoch: 179 \tLoss: 0.011163\n",
      "Train fp Epoch: 180 \tLoss: 0.011163\n",
      "Train fp Epoch: 181 \tLoss: 0.011162\n",
      "Train fp Epoch: 182 \tLoss: 0.011162\n",
      "Train fp Epoch: 183 \tLoss: 0.011162\n",
      "Train fp Epoch: 184 \tLoss: 0.011162\n",
      "Train fp Epoch: 185 \tLoss: 0.011161\n",
      "Train fp Epoch: 186 \tLoss: 0.011161\n",
      "Train fp Epoch: 187 \tLoss: 0.011161\n",
      "Train fp Epoch: 188 \tLoss: 0.011161\n",
      "Train fp Epoch: 189 \tLoss: 0.011161\n",
      "Train fp Epoch: 190 \tLoss: 0.011160\n",
      "Train fp Epoch: 191 \tLoss: 0.011160\n",
      "Train fp Epoch: 192 \tLoss: 0.011160\n",
      "Train fp Epoch: 193 \tLoss: 0.011160\n",
      "Train fp Epoch: 194 \tLoss: 0.011160\n",
      "Train fp Epoch: 195 \tLoss: 0.011159\n",
      "Train fp Epoch: 196 \tLoss: 0.011159\n",
      "Train fp Epoch: 197 \tLoss: 0.011159\n",
      "Train fp Epoch: 198 \tLoss: 0.011159\n",
      "Train fp Epoch: 199 \tLoss: 0.011159\n",
      "Train fp Epoch: 200 \tLoss: 0.011159\n",
      "test fp MSE: 0.028216\n",
      "test fp MAE: 0.141161\n",
      "test fp Loss: 0.014080\n",
      "开始备份网络权重\n",
      "开始处理草药权重矩阵\n",
      "开始处理化合物权重矩阵\n",
      "\n",
      "开始处理基因权重矩阵\n",
      "\n",
      "开始处理通路权重矩阵\n",
      "\n",
      "开始处理功能权重矩阵\n",
      "第8个药方\n",
      "完成数据切分\n",
      "完成数据读取，开始稀疏化网络\n",
      "开始处理草药权重矩阵\n",
      "开始处理化合物权重矩阵\n",
      "\n",
      "开始处理基因权重矩阵\n",
      "\n",
      "开始处理通路权重矩阵\n",
      "\n",
      "开始处理功能权重矩阵\n",
      "开始训练和测试\n",
      "Train fp Epoch: 1 \tLoss: 0.020864\n",
      "Train fp Epoch: 2 \tLoss: 0.020773\n",
      "Train fp Epoch: 3 \tLoss: 0.020681\n",
      "Train fp Epoch: 4 \tLoss: 0.020592\n",
      "Train fp Epoch: 5 \tLoss: 0.020508\n",
      "Train fp Epoch: 6 \tLoss: 0.020426\n",
      "Train fp Epoch: 7 \tLoss: 0.020348\n",
      "Train fp Epoch: 8 \tLoss: 0.020273\n",
      "Train fp Epoch: 9 \tLoss: 0.020201\n",
      "Train fp Epoch: 10 \tLoss: 0.020132\n",
      "Train fp Epoch: 11 \tLoss: 0.020066\n",
      "Train fp Epoch: 12 \tLoss: 0.020003\n",
      "Train fp Epoch: 13 \tLoss: 0.019942\n",
      "Train fp Epoch: 14 \tLoss: 0.019883\n",
      "Train fp Epoch: 15 \tLoss: 0.019827\n",
      "Train fp Epoch: 16 \tLoss: 0.019773\n",
      "Train fp Epoch: 17 \tLoss: 0.019721\n",
      "Train fp Epoch: 18 \tLoss: 0.019671\n",
      "Train fp Epoch: 19 \tLoss: 0.019623\n",
      "Train fp Epoch: 20 \tLoss: 0.019577\n",
      "Train fp Epoch: 21 \tLoss: 0.019533\n",
      "Train fp Epoch: 22 \tLoss: 0.019491\n",
      "Train fp Epoch: 23 \tLoss: 0.019450\n",
      "Train fp Epoch: 24 \tLoss: 0.019411\n",
      "Train fp Epoch: 25 \tLoss: 0.019373\n",
      "Train fp Epoch: 26 \tLoss: 0.019337\n",
      "Train fp Epoch: 27 \tLoss: 0.019302\n",
      "Train fp Epoch: 28 \tLoss: 0.019268\n",
      "Train fp Epoch: 29 \tLoss: 0.019236\n",
      "Train fp Epoch: 30 \tLoss: 0.019205\n",
      "Train fp Epoch: 31 \tLoss: 0.019175\n",
      "Train fp Epoch: 32 \tLoss: 0.019147\n",
      "Train fp Epoch: 33 \tLoss: 0.019119\n",
      "Train fp Epoch: 34 \tLoss: 0.019093\n",
      "Train fp Epoch: 35 \tLoss: 0.019067\n",
      "Train fp Epoch: 36 \tLoss: 0.019043\n",
      "Train fp Epoch: 37 \tLoss: 0.019019\n",
      "Train fp Epoch: 38 \tLoss: 0.018996\n",
      "Train fp Epoch: 39 \tLoss: 0.018975\n",
      "Train fp Epoch: 40 \tLoss: 0.018954\n",
      "Train fp Epoch: 41 \tLoss: 0.018933\n",
      "Train fp Epoch: 42 \tLoss: 0.018914\n",
      "Train fp Epoch: 43 \tLoss: 0.018895\n",
      "Train fp Epoch: 44 \tLoss: 0.018877\n",
      "Train fp Epoch: 45 \tLoss: 0.018860\n",
      "Train fp Epoch: 46 \tLoss: 0.018843\n",
      "Train fp Epoch: 47 \tLoss: 0.018827\n",
      "Train fp Epoch: 48 \tLoss: 0.018812\n",
      "Train fp Epoch: 49 \tLoss: 0.018797\n",
      "Train fp Epoch: 50 \tLoss: 0.018783\n",
      "Train fp Epoch: 51 \tLoss: 0.018769\n",
      "Train fp Epoch: 52 \tLoss: 0.018756\n",
      "Train fp Epoch: 53 \tLoss: 0.018743\n",
      "Train fp Epoch: 54 \tLoss: 0.018731\n",
      "Train fp Epoch: 55 \tLoss: 0.018719\n",
      "Train fp Epoch: 56 \tLoss: 0.018707\n",
      "Train fp Epoch: 57 \tLoss: 0.018696\n",
      "Train fp Epoch: 58 \tLoss: 0.018686\n",
      "Train fp Epoch: 59 \tLoss: 0.018676\n",
      "Train fp Epoch: 60 \tLoss: 0.018666\n",
      "Train fp Epoch: 61 \tLoss: 0.018656\n",
      "Train fp Epoch: 62 \tLoss: 0.018647\n",
      "Train fp Epoch: 63 \tLoss: 0.018638\n",
      "Train fp Epoch: 64 \tLoss: 0.018630\n",
      "Train fp Epoch: 65 \tLoss: 0.018622\n",
      "Train fp Epoch: 66 \tLoss: 0.018614\n",
      "Train fp Epoch: 67 \tLoss: 0.018606\n",
      "Train fp Epoch: 68 \tLoss: 0.018599\n",
      "Train fp Epoch: 69 \tLoss: 0.018592\n",
      "Train fp Epoch: 70 \tLoss: 0.018585\n",
      "Train fp Epoch: 71 \tLoss: 0.018579\n",
      "Train fp Epoch: 72 \tLoss: 0.018573\n",
      "Train fp Epoch: 73 \tLoss: 0.018567\n",
      "Train fp Epoch: 74 \tLoss: 0.018561\n",
      "Train fp Epoch: 75 \tLoss: 0.018555\n",
      "Train fp Epoch: 76 \tLoss: 0.018550\n",
      "Train fp Epoch: 77 \tLoss: 0.018545\n",
      "Train fp Epoch: 78 \tLoss: 0.018540\n",
      "Train fp Epoch: 79 \tLoss: 0.018535\n",
      "Train fp Epoch: 80 \tLoss: 0.018530\n",
      "Train fp Epoch: 81 \tLoss: 0.018526\n",
      "Train fp Epoch: 82 \tLoss: 0.018521\n",
      "Train fp Epoch: 83 \tLoss: 0.018517\n",
      "Train fp Epoch: 84 \tLoss: 0.018513\n",
      "Train fp Epoch: 85 \tLoss: 0.018509\n",
      "Train fp Epoch: 86 \tLoss: 0.018506\n",
      "Train fp Epoch: 87 \tLoss: 0.018502\n",
      "Train fp Epoch: 88 \tLoss: 0.018499\n",
      "Train fp Epoch: 89 \tLoss: 0.018495\n",
      "Train fp Epoch: 90 \tLoss: 0.018492\n",
      "Train fp Epoch: 91 \tLoss: 0.018489\n",
      "Train fp Epoch: 92 \tLoss: 0.018486\n",
      "Train fp Epoch: 93 \tLoss: 0.018483\n",
      "Train fp Epoch: 94 \tLoss: 0.018480\n",
      "Train fp Epoch: 95 \tLoss: 0.018478\n",
      "Train fp Epoch: 96 \tLoss: 0.018475\n",
      "Train fp Epoch: 97 \tLoss: 0.018473\n",
      "Train fp Epoch: 98 \tLoss: 0.018470\n",
      "Train fp Epoch: 99 \tLoss: 0.018468\n",
      "Train fp Epoch: 100 \tLoss: 0.018466\n",
      "Train fp Epoch: 101 \tLoss: 0.018463\n",
      "Train fp Epoch: 102 \tLoss: 0.018461\n",
      "Train fp Epoch: 103 \tLoss: 0.018459\n",
      "Train fp Epoch: 104 \tLoss: 0.018457\n",
      "Train fp Epoch: 105 \tLoss: 0.018456\n",
      "Train fp Epoch: 106 \tLoss: 0.018454\n",
      "Train fp Epoch: 107 \tLoss: 0.018452\n",
      "Train fp Epoch: 108 \tLoss: 0.018450\n",
      "Train fp Epoch: 109 \tLoss: 0.018449\n",
      "Train fp Epoch: 110 \tLoss: 0.018447\n",
      "Train fp Epoch: 111 \tLoss: 0.018446\n",
      "Train fp Epoch: 112 \tLoss: 0.018444\n",
      "Train fp Epoch: 113 \tLoss: 0.018443\n",
      "Train fp Epoch: 114 \tLoss: 0.018441\n",
      "Train fp Epoch: 115 \tLoss: 0.018440\n",
      "Train fp Epoch: 116 \tLoss: 0.018439\n",
      "Train fp Epoch: 117 \tLoss: 0.018438\n",
      "Train fp Epoch: 118 \tLoss: 0.018436\n",
      "Train fp Epoch: 119 \tLoss: 0.018435\n",
      "Train fp Epoch: 120 \tLoss: 0.018434\n",
      "Train fp Epoch: 121 \tLoss: 0.018433\n",
      "Train fp Epoch: 122 \tLoss: 0.018432\n",
      "Train fp Epoch: 123 \tLoss: 0.018431\n",
      "Train fp Epoch: 124 \tLoss: 0.018430\n",
      "Train fp Epoch: 125 \tLoss: 0.018429\n",
      "Train fp Epoch: 126 \tLoss: 0.018428\n",
      "Train fp Epoch: 127 \tLoss: 0.018428\n",
      "Train fp Epoch: 128 \tLoss: 0.018427\n",
      "Train fp Epoch: 129 \tLoss: 0.018426\n",
      "Train fp Epoch: 130 \tLoss: 0.018425\n",
      "Train fp Epoch: 131 \tLoss: 0.018424\n",
      "Train fp Epoch: 132 \tLoss: 0.018424\n",
      "Train fp Epoch: 133 \tLoss: 0.018423\n",
      "Train fp Epoch: 134 \tLoss: 0.018422\n",
      "Train fp Epoch: 135 \tLoss: 0.018422\n",
      "Train fp Epoch: 136 \tLoss: 0.018421\n",
      "Train fp Epoch: 137 \tLoss: 0.018420\n",
      "Train fp Epoch: 138 \tLoss: 0.018420\n",
      "Train fp Epoch: 139 \tLoss: 0.018419\n",
      "Train fp Epoch: 140 \tLoss: 0.018419\n",
      "Train fp Epoch: 141 \tLoss: 0.018418\n",
      "Train fp Epoch: 142 \tLoss: 0.018418\n",
      "Train fp Epoch: 143 \tLoss: 0.018417\n",
      "Train fp Epoch: 144 \tLoss: 0.018417\n",
      "Train fp Epoch: 145 \tLoss: 0.018416\n",
      "Train fp Epoch: 146 \tLoss: 0.018416\n",
      "Train fp Epoch: 147 \tLoss: 0.018415\n",
      "Train fp Epoch: 148 \tLoss: 0.018415\n",
      "Train fp Epoch: 149 \tLoss: 0.018414\n",
      "Train fp Epoch: 150 \tLoss: 0.018414\n",
      "Train fp Epoch: 151 \tLoss: 0.018414\n",
      "Train fp Epoch: 152 \tLoss: 0.018413\n",
      "Train fp Epoch: 153 \tLoss: 0.018413\n",
      "Train fp Epoch: 154 \tLoss: 0.018413\n",
      "Train fp Epoch: 155 \tLoss: 0.018412\n",
      "Train fp Epoch: 156 \tLoss: 0.018412\n",
      "Train fp Epoch: 157 \tLoss: 0.018412\n",
      "Train fp Epoch: 158 \tLoss: 0.018411\n",
      "Train fp Epoch: 159 \tLoss: 0.018411\n",
      "Train fp Epoch: 160 \tLoss: 0.018411\n",
      "Train fp Epoch: 161 \tLoss: 0.018410\n",
      "Train fp Epoch: 162 \tLoss: 0.018410\n",
      "Train fp Epoch: 163 \tLoss: 0.018410\n",
      "Train fp Epoch: 164 \tLoss: 0.018410\n",
      "Train fp Epoch: 165 \tLoss: 0.018409\n",
      "Train fp Epoch: 166 \tLoss: 0.018409\n",
      "Train fp Epoch: 167 \tLoss: 0.018409\n",
      "Train fp Epoch: 168 \tLoss: 0.018409\n",
      "Train fp Epoch: 169 \tLoss: 0.018408\n",
      "Train fp Epoch: 170 \tLoss: 0.018408\n",
      "Train fp Epoch: 171 \tLoss: 0.018408\n",
      "Train fp Epoch: 172 \tLoss: 0.018408\n",
      "Train fp Epoch: 173 \tLoss: 0.018408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fp Epoch: 174 \tLoss: 0.018407\n",
      "Train fp Epoch: 175 \tLoss: 0.018407\n",
      "Train fp Epoch: 176 \tLoss: 0.018407\n",
      "Train fp Epoch: 177 \tLoss: 0.018407\n",
      "Train fp Epoch: 178 \tLoss: 0.018407\n",
      "Train fp Epoch: 179 \tLoss: 0.018407\n",
      "Train fp Epoch: 180 \tLoss: 0.018406\n",
      "Train fp Epoch: 181 \tLoss: 0.018406\n",
      "Train fp Epoch: 182 \tLoss: 0.018406\n",
      "Train fp Epoch: 183 \tLoss: 0.018406\n",
      "Train fp Epoch: 184 \tLoss: 0.018406\n",
      "Train fp Epoch: 185 \tLoss: 0.018406\n",
      "Train fp Epoch: 186 \tLoss: 0.018406\n",
      "Train fp Epoch: 187 \tLoss: 0.018405\n",
      "Train fp Epoch: 188 \tLoss: 0.018405\n",
      "Train fp Epoch: 189 \tLoss: 0.018405\n",
      "Train fp Epoch: 190 \tLoss: 0.018405\n",
      "Train fp Epoch: 191 \tLoss: 0.018405\n",
      "Train fp Epoch: 192 \tLoss: 0.018405\n",
      "Train fp Epoch: 193 \tLoss: 0.018405\n",
      "Train fp Epoch: 194 \tLoss: 0.018404\n",
      "Train fp Epoch: 195 \tLoss: 0.018404\n",
      "Train fp Epoch: 196 \tLoss: 0.018404\n",
      "Train fp Epoch: 197 \tLoss: 0.018404\n",
      "Train fp Epoch: 198 \tLoss: 0.018404\n",
      "Train fp Epoch: 199 \tLoss: 0.018404\n",
      "Train fp Epoch: 200 \tLoss: 0.018404\n",
      "test fp MSE: 0.038428\n",
      "test fp MAE: 0.161944\n",
      "test fp Loss: 0.019466\n",
      "开始备份网络权重\n",
      "开始处理草药权重矩阵\n",
      "开始处理化合物权重矩阵\n",
      "\n",
      "开始处理基因权重矩阵\n",
      "\n",
      "开始处理通路权重矩阵\n",
      "\n",
      "开始处理功能权重矩阵\n",
      "第9个药方\n",
      "完成数据切分\n",
      "完成数据读取，开始稀疏化网络\n",
      "开始处理草药权重矩阵\n",
      "开始处理化合物权重矩阵\n",
      "\n",
      "开始处理基因权重矩阵\n",
      "\n",
      "开始处理通路权重矩阵\n",
      "\n",
      "开始处理功能权重矩阵\n",
      "开始训练和测试\n",
      "Train fp Epoch: 1 \tLoss: 0.039828\n",
      "Train fp Epoch: 2 \tLoss: 0.038594\n",
      "Train fp Epoch: 3 \tLoss: 0.037329\n",
      "Train fp Epoch: 4 \tLoss: 0.036105\n",
      "Train fp Epoch: 5 \tLoss: 0.034922\n",
      "Train fp Epoch: 6 \tLoss: 0.033777\n",
      "Train fp Epoch: 7 \tLoss: 0.032671\n",
      "Train fp Epoch: 8 \tLoss: 0.031600\n",
      "Train fp Epoch: 9 \tLoss: 0.030564\n",
      "Train fp Epoch: 10 \tLoss: 0.029562\n",
      "Train fp Epoch: 11 \tLoss: 0.028592\n",
      "Train fp Epoch: 12 \tLoss: 0.027654\n",
      "Train fp Epoch: 13 \tLoss: 0.026746\n",
      "Train fp Epoch: 14 \tLoss: 0.025868\n",
      "Train fp Epoch: 15 \tLoss: 0.025019\n",
      "Train fp Epoch: 16 \tLoss: 0.024198\n",
      "Train fp Epoch: 17 \tLoss: 0.023404\n",
      "Train fp Epoch: 18 \tLoss: 0.022636\n",
      "Train fp Epoch: 19 \tLoss: 0.021895\n",
      "Train fp Epoch: 20 \tLoss: 0.021178\n",
      "Train fp Epoch: 21 \tLoss: 0.020486\n",
      "Train fp Epoch: 22 \tLoss: 0.019818\n",
      "Train fp Epoch: 23 \tLoss: 0.019174\n",
      "Train fp Epoch: 24 \tLoss: 0.018552\n",
      "Train fp Epoch: 25 \tLoss: 0.017951\n",
      "Train fp Epoch: 26 \tLoss: 0.017373\n",
      "Train fp Epoch: 27 \tLoss: 0.016815\n",
      "Train fp Epoch: 28 \tLoss: 0.016278\n",
      "Train fp Epoch: 29 \tLoss: 0.015761\n",
      "Train fp Epoch: 30 \tLoss: 0.015263\n",
      "Train fp Epoch: 31 \tLoss: 0.014784\n",
      "Train fp Epoch: 32 \tLoss: 0.014323\n",
      "Train fp Epoch: 33 \tLoss: 0.013880\n",
      "Train fp Epoch: 34 \tLoss: 0.013453\n",
      "Train fp Epoch: 35 \tLoss: 0.013044\n",
      "Train fp Epoch: 36 \tLoss: 0.012651\n",
      "Train fp Epoch: 37 \tLoss: 0.012274\n",
      "Train fp Epoch: 38 \tLoss: 0.011911\n",
      "Train fp Epoch: 39 \tLoss: 0.011564\n",
      "Train fp Epoch: 40 \tLoss: 0.011230\n",
      "Train fp Epoch: 41 \tLoss: 0.010911\n",
      "Train fp Epoch: 42 \tLoss: 0.010604\n",
      "Train fp Epoch: 43 \tLoss: 0.010311\n",
      "Train fp Epoch: 44 \tLoss: 0.010030\n",
      "Train fp Epoch: 45 \tLoss: 0.009760\n",
      "Train fp Epoch: 46 \tLoss: 0.009502\n",
      "Train fp Epoch: 47 \tLoss: 0.009255\n",
      "Train fp Epoch: 48 \tLoss: 0.009019\n",
      "Train fp Epoch: 49 \tLoss: 0.008793\n",
      "Train fp Epoch: 50 \tLoss: 0.008576\n",
      "Train fp Epoch: 51 \tLoss: 0.008369\n",
      "Train fp Epoch: 52 \tLoss: 0.008171\n",
      "Train fp Epoch: 53 \tLoss: 0.007982\n",
      "Train fp Epoch: 54 \tLoss: 0.007801\n",
      "Train fp Epoch: 55 \tLoss: 0.007628\n",
      "Train fp Epoch: 56 \tLoss: 0.007463\n",
      "Train fp Epoch: 57 \tLoss: 0.007304\n",
      "Train fp Epoch: 58 \tLoss: 0.007153\n",
      "Train fp Epoch: 59 \tLoss: 0.007009\n",
      "Train fp Epoch: 60 \tLoss: 0.006871\n",
      "Train fp Epoch: 61 \tLoss: 0.006739\n",
      "Train fp Epoch: 62 \tLoss: 0.006614\n",
      "Train fp Epoch: 63 \tLoss: 0.006493\n",
      "Train fp Epoch: 64 \tLoss: 0.006378\n",
      "Train fp Epoch: 65 \tLoss: 0.006269\n",
      "Train fp Epoch: 66 \tLoss: 0.006164\n",
      "Train fp Epoch: 67 \tLoss: 0.006063\n",
      "Train fp Epoch: 68 \tLoss: 0.005968\n",
      "Train fp Epoch: 69 \tLoss: 0.005876\n",
      "Train fp Epoch: 70 \tLoss: 0.005789\n",
      "Train fp Epoch: 71 \tLoss: 0.005705\n",
      "Train fp Epoch: 72 \tLoss: 0.005625\n",
      "Train fp Epoch: 73 \tLoss: 0.005549\n",
      "Train fp Epoch: 74 \tLoss: 0.005476\n",
      "Train fp Epoch: 75 \tLoss: 0.005406\n",
      "Train fp Epoch: 76 \tLoss: 0.005340\n",
      "Train fp Epoch: 77 \tLoss: 0.005276\n",
      "Train fp Epoch: 78 \tLoss: 0.005215\n",
      "Train fp Epoch: 79 \tLoss: 0.005157\n",
      "Train fp Epoch: 80 \tLoss: 0.005101\n",
      "Train fp Epoch: 81 \tLoss: 0.005048\n",
      "Train fp Epoch: 82 \tLoss: 0.004997\n",
      "Train fp Epoch: 83 \tLoss: 0.004949\n",
      "Train fp Epoch: 84 \tLoss: 0.004902\n",
      "Train fp Epoch: 85 \tLoss: 0.004857\n",
      "Train fp Epoch: 86 \tLoss: 0.004815\n",
      "Train fp Epoch: 87 \tLoss: 0.004774\n",
      "Train fp Epoch: 88 \tLoss: 0.004735\n",
      "Train fp Epoch: 89 \tLoss: 0.004697\n",
      "Train fp Epoch: 90 \tLoss: 0.004662\n",
      "Train fp Epoch: 91 \tLoss: 0.004627\n",
      "Train fp Epoch: 92 \tLoss: 0.004595\n",
      "Train fp Epoch: 93 \tLoss: 0.004563\n",
      "Train fp Epoch: 94 \tLoss: 0.004533\n",
      "Train fp Epoch: 95 \tLoss: 0.004504\n",
      "Train fp Epoch: 96 \tLoss: 0.004477\n",
      "Train fp Epoch: 97 \tLoss: 0.004450\n",
      "Train fp Epoch: 98 \tLoss: 0.004425\n",
      "Train fp Epoch: 99 \tLoss: 0.004400\n",
      "Train fp Epoch: 100 \tLoss: 0.004377\n",
      "Train fp Epoch: 101 \tLoss: 0.004355\n",
      "Train fp Epoch: 102 \tLoss: 0.004333\n",
      "Train fp Epoch: 103 \tLoss: 0.004313\n",
      "Train fp Epoch: 104 \tLoss: 0.004293\n",
      "Train fp Epoch: 105 \tLoss: 0.004274\n",
      "Train fp Epoch: 106 \tLoss: 0.004256\n",
      "Train fp Epoch: 107 \tLoss: 0.004239\n",
      "Train fp Epoch: 108 \tLoss: 0.004222\n",
      "Train fp Epoch: 109 \tLoss: 0.004206\n",
      "Train fp Epoch: 110 \tLoss: 0.004191\n",
      "Train fp Epoch: 111 \tLoss: 0.004176\n",
      "Train fp Epoch: 112 \tLoss: 0.004162\n",
      "Train fp Epoch: 113 \tLoss: 0.004148\n",
      "Train fp Epoch: 114 \tLoss: 0.004135\n",
      "Train fp Epoch: 115 \tLoss: 0.004122\n",
      "Train fp Epoch: 116 \tLoss: 0.004110\n",
      "Train fp Epoch: 117 \tLoss: 0.004099\n",
      "Train fp Epoch: 118 \tLoss: 0.004088\n",
      "Train fp Epoch: 119 \tLoss: 0.004077\n",
      "Train fp Epoch: 120 \tLoss: 0.004067\n",
      "Train fp Epoch: 121 \tLoss: 0.004057\n",
      "Train fp Epoch: 122 \tLoss: 0.004047\n",
      "Train fp Epoch: 123 \tLoss: 0.004038\n",
      "Train fp Epoch: 124 \tLoss: 0.004029\n",
      "Train fp Epoch: 125 \tLoss: 0.004021\n",
      "Train fp Epoch: 126 \tLoss: 0.004012\n",
      "Train fp Epoch: 127 \tLoss: 0.004005\n",
      "Train fp Epoch: 128 \tLoss: 0.003997\n",
      "Train fp Epoch: 129 \tLoss: 0.003990\n",
      "Train fp Epoch: 130 \tLoss: 0.003983\n",
      "Train fp Epoch: 131 \tLoss: 0.003976\n",
      "Train fp Epoch: 132 \tLoss: 0.003969\n",
      "Train fp Epoch: 133 \tLoss: 0.003963\n",
      "Train fp Epoch: 134 \tLoss: 0.003957\n",
      "Train fp Epoch: 135 \tLoss: 0.003951\n",
      "Train fp Epoch: 136 \tLoss: 0.003946\n",
      "Train fp Epoch: 137 \tLoss: 0.003940\n",
      "Train fp Epoch: 138 \tLoss: 0.003935\n",
      "Train fp Epoch: 139 \tLoss: 0.003930\n",
      "Train fp Epoch: 140 \tLoss: 0.003925\n",
      "Train fp Epoch: 141 \tLoss: 0.003920\n",
      "Train fp Epoch: 142 \tLoss: 0.003916\n",
      "Train fp Epoch: 143 \tLoss: 0.003912\n",
      "Train fp Epoch: 144 \tLoss: 0.003907\n",
      "Train fp Epoch: 145 \tLoss: 0.003903\n",
      "Train fp Epoch: 146 \tLoss: 0.003899\n",
      "Train fp Epoch: 147 \tLoss: 0.003896\n",
      "Train fp Epoch: 148 \tLoss: 0.003892\n",
      "Train fp Epoch: 149 \tLoss: 0.003888\n",
      "Train fp Epoch: 150 \tLoss: 0.003885\n",
      "Train fp Epoch: 151 \tLoss: 0.003882\n",
      "Train fp Epoch: 152 \tLoss: 0.003878\n",
      "Train fp Epoch: 153 \tLoss: 0.003875\n",
      "Train fp Epoch: 154 \tLoss: 0.003872\n",
      "Train fp Epoch: 155 \tLoss: 0.003869\n",
      "Train fp Epoch: 156 \tLoss: 0.003867\n",
      "Train fp Epoch: 157 \tLoss: 0.003864\n",
      "Train fp Epoch: 158 \tLoss: 0.003861\n",
      "Train fp Epoch: 159 \tLoss: 0.003859\n",
      "Train fp Epoch: 160 \tLoss: 0.003856\n",
      "Train fp Epoch: 161 \tLoss: 0.003854\n",
      "Train fp Epoch: 162 \tLoss: 0.003852\n",
      "Train fp Epoch: 163 \tLoss: 0.003849\n",
      "Train fp Epoch: 164 \tLoss: 0.003847\n",
      "Train fp Epoch: 165 \tLoss: 0.003845\n",
      "Train fp Epoch: 166 \tLoss: 0.003843\n",
      "Train fp Epoch: 167 \tLoss: 0.003841\n",
      "Train fp Epoch: 168 \tLoss: 0.003839\n",
      "Train fp Epoch: 169 \tLoss: 0.003837\n",
      "Train fp Epoch: 170 \tLoss: 0.003835\n",
      "Train fp Epoch: 171 \tLoss: 0.003834\n",
      "Train fp Epoch: 172 \tLoss: 0.003832\n",
      "Train fp Epoch: 173 \tLoss: 0.003830\n",
      "Train fp Epoch: 174 \tLoss: 0.003829\n",
      "Train fp Epoch: 175 \tLoss: 0.003827\n",
      "Train fp Epoch: 176 \tLoss: 0.003826\n",
      "Train fp Epoch: 177 \tLoss: 0.003824\n",
      "Train fp Epoch: 178 \tLoss: 0.003823\n",
      "Train fp Epoch: 179 \tLoss: 0.003821\n",
      "Train fp Epoch: 180 \tLoss: 0.003820\n",
      "Train fp Epoch: 181 \tLoss: 0.003819\n",
      "Train fp Epoch: 182 \tLoss: 0.003817\n",
      "Train fp Epoch: 183 \tLoss: 0.003816\n",
      "Train fp Epoch: 184 \tLoss: 0.003815\n",
      "Train fp Epoch: 185 \tLoss: 0.003814\n",
      "Train fp Epoch: 186 \tLoss: 0.003812\n",
      "Train fp Epoch: 187 \tLoss: 0.003811\n",
      "Train fp Epoch: 188 \tLoss: 0.003810\n",
      "Train fp Epoch: 189 \tLoss: 0.003809\n",
      "Train fp Epoch: 190 \tLoss: 0.003808\n",
      "Train fp Epoch: 191 \tLoss: 0.003807\n",
      "Train fp Epoch: 192 \tLoss: 0.003806\n",
      "Train fp Epoch: 193 \tLoss: 0.003805\n",
      "Train fp Epoch: 194 \tLoss: 0.003804\n",
      "Train fp Epoch: 195 \tLoss: 0.003803\n",
      "Train fp Epoch: 196 \tLoss: 0.003802\n",
      "Train fp Epoch: 197 \tLoss: 0.003801\n",
      "Train fp Epoch: 198 \tLoss: 0.003801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fp Epoch: 199 \tLoss: 0.003800\n",
      "Train fp Epoch: 200 \tLoss: 0.003799\n",
      "test fp MSE: 0.003552\n",
      "test fp MAE: 0.048080\n",
      "test fp Loss: 0.001688\n",
      "开始备份网络权重\n",
      "开始处理草药权重矩阵\n",
      "开始处理化合物权重矩阵\n",
      "\n",
      "开始处理基因权重矩阵\n",
      "\n",
      "开始处理通路权重矩阵\n",
      "\n",
      "开始处理功能权重矩阵\n",
      "第10个药方\n",
      "完成数据切分\n",
      "完成数据读取，开始稀疏化网络\n",
      "开始处理草药权重矩阵\n",
      "开始处理化合物权重矩阵\n",
      "\n",
      "开始处理基因权重矩阵\n",
      "\n",
      "开始处理通路权重矩阵\n",
      "\n",
      "开始处理功能权重矩阵\n",
      "开始训练和测试\n",
      "Train fp Epoch: 1 \tLoss: 0.069109\n",
      "Train fp Epoch: 2 \tLoss: 0.067454\n",
      "Train fp Epoch: 3 \tLoss: 0.065729\n",
      "Train fp Epoch: 4 \tLoss: 0.064041\n",
      "Train fp Epoch: 5 \tLoss: 0.062395\n",
      "Train fp Epoch: 6 \tLoss: 0.060795\n",
      "Train fp Epoch: 7 \tLoss: 0.059244\n",
      "Train fp Epoch: 8 \tLoss: 0.057744\n",
      "Train fp Epoch: 9 \tLoss: 0.056297\n",
      "Train fp Epoch: 10 \tLoss: 0.054904\n",
      "Train fp Epoch: 11 \tLoss: 0.053567\n",
      "Train fp Epoch: 12 \tLoss: 0.052286\n",
      "Train fp Epoch: 13 \tLoss: 0.051058\n",
      "Train fp Epoch: 14 \tLoss: 0.049885\n",
      "Train fp Epoch: 15 \tLoss: 0.048764\n",
      "Train fp Epoch: 16 \tLoss: 0.047694\n",
      "Train fp Epoch: 17 \tLoss: 0.046674\n",
      "Train fp Epoch: 18 \tLoss: 0.045703\n",
      "Train fp Epoch: 19 \tLoss: 0.044777\n",
      "Train fp Epoch: 20 \tLoss: 0.043895\n",
      "Train fp Epoch: 21 \tLoss: 0.043056\n",
      "Train fp Epoch: 22 \tLoss: 0.042257\n",
      "Train fp Epoch: 23 \tLoss: 0.041496\n",
      "Train fp Epoch: 24 \tLoss: 0.040772\n",
      "Train fp Epoch: 25 \tLoss: 0.040082\n",
      "Train fp Epoch: 26 \tLoss: 0.039426\n",
      "Train fp Epoch: 27 \tLoss: 0.038801\n",
      "Train fp Epoch: 28 \tLoss: 0.038206\n",
      "Train fp Epoch: 29 \tLoss: 0.037640\n",
      "Train fp Epoch: 30 \tLoss: 0.037100\n",
      "Train fp Epoch: 31 \tLoss: 0.036585\n",
      "Train fp Epoch: 32 \tLoss: 0.036094\n",
      "Train fp Epoch: 33 \tLoss: 0.035626\n",
      "Train fp Epoch: 34 \tLoss: 0.035180\n",
      "Train fp Epoch: 35 \tLoss: 0.034754\n",
      "Train fp Epoch: 36 \tLoss: 0.034348\n",
      "Train fp Epoch: 37 \tLoss: 0.033959\n",
      "Train fp Epoch: 38 \tLoss: 0.033589\n",
      "Train fp Epoch: 39 \tLoss: 0.033234\n",
      "Train fp Epoch: 40 \tLoss: 0.032896\n",
      "Train fp Epoch: 41 \tLoss: 0.032572\n",
      "Train fp Epoch: 42 \tLoss: 0.032263\n",
      "Train fp Epoch: 43 \tLoss: 0.031966\n",
      "Train fp Epoch: 44 \tLoss: 0.031683\n",
      "Train fp Epoch: 45 \tLoss: 0.031412\n",
      "Train fp Epoch: 46 \tLoss: 0.031152\n",
      "Train fp Epoch: 47 \tLoss: 0.030904\n",
      "Train fp Epoch: 48 \tLoss: 0.030665\n",
      "Train fp Epoch: 49 \tLoss: 0.030437\n",
      "Train fp Epoch: 50 \tLoss: 0.030219\n",
      "Train fp Epoch: 51 \tLoss: 0.030009\n",
      "Train fp Epoch: 52 \tLoss: 0.029808\n",
      "Train fp Epoch: 53 \tLoss: 0.029615\n",
      "Train fp Epoch: 54 \tLoss: 0.029430\n",
      "Train fp Epoch: 55 \tLoss: 0.029253\n",
      "Train fp Epoch: 56 \tLoss: 0.029083\n",
      "Train fp Epoch: 57 \tLoss: 0.028919\n",
      "Train fp Epoch: 58 \tLoss: 0.028762\n",
      "Train fp Epoch: 59 \tLoss: 0.028612\n",
      "Train fp Epoch: 60 \tLoss: 0.028467\n",
      "Train fp Epoch: 61 \tLoss: 0.028328\n",
      "Train fp Epoch: 62 \tLoss: 0.028195\n",
      "Train fp Epoch: 63 \tLoss: 0.028066\n",
      "Train fp Epoch: 64 \tLoss: 0.027943\n",
      "Train fp Epoch: 65 \tLoss: 0.027825\n",
      "Train fp Epoch: 66 \tLoss: 0.027711\n",
      "Train fp Epoch: 67 \tLoss: 0.027601\n",
      "Train fp Epoch: 68 \tLoss: 0.027496\n",
      "Train fp Epoch: 69 \tLoss: 0.027395\n",
      "Train fp Epoch: 70 \tLoss: 0.027298\n",
      "Train fp Epoch: 71 \tLoss: 0.027204\n",
      "Train fp Epoch: 72 \tLoss: 0.027114\n",
      "Train fp Epoch: 73 \tLoss: 0.027027\n",
      "Train fp Epoch: 74 \tLoss: 0.026944\n",
      "Train fp Epoch: 75 \tLoss: 0.026864\n",
      "Train fp Epoch: 76 \tLoss: 0.026787\n",
      "Train fp Epoch: 77 \tLoss: 0.026712\n",
      "Train fp Epoch: 78 \tLoss: 0.026641\n",
      "Train fp Epoch: 79 \tLoss: 0.026572\n",
      "Train fp Epoch: 80 \tLoss: 0.026506\n",
      "Train fp Epoch: 81 \tLoss: 0.026442\n",
      "Train fp Epoch: 82 \tLoss: 0.026380\n",
      "Train fp Epoch: 83 \tLoss: 0.026321\n",
      "Train fp Epoch: 84 \tLoss: 0.026264\n",
      "Train fp Epoch: 85 \tLoss: 0.026209\n",
      "Train fp Epoch: 86 \tLoss: 0.026156\n",
      "Train fp Epoch: 87 \tLoss: 0.026106\n",
      "Train fp Epoch: 88 \tLoss: 0.026057\n",
      "Train fp Epoch: 89 \tLoss: 0.026009\n",
      "Train fp Epoch: 90 \tLoss: 0.025964\n",
      "Train fp Epoch: 91 \tLoss: 0.025920\n",
      "Train fp Epoch: 92 \tLoss: 0.025878\n",
      "Train fp Epoch: 93 \tLoss: 0.025837\n",
      "Train fp Epoch: 94 \tLoss: 0.025798\n",
      "Train fp Epoch: 95 \tLoss: 0.025760\n",
      "Train fp Epoch: 96 \tLoss: 0.025724\n",
      "Train fp Epoch: 97 \tLoss: 0.025689\n",
      "Train fp Epoch: 98 \tLoss: 0.025655\n",
      "Train fp Epoch: 99 \tLoss: 0.025623\n",
      "Train fp Epoch: 100 \tLoss: 0.025591\n",
      "Train fp Epoch: 101 \tLoss: 0.025561\n",
      "Train fp Epoch: 102 \tLoss: 0.025532\n",
      "Train fp Epoch: 103 \tLoss: 0.025504\n",
      "Train fp Epoch: 104 \tLoss: 0.025477\n",
      "Train fp Epoch: 105 \tLoss: 0.025451\n",
      "Train fp Epoch: 106 \tLoss: 0.025426\n",
      "Train fp Epoch: 107 \tLoss: 0.025401\n",
      "Train fp Epoch: 108 \tLoss: 0.025378\n",
      "Train fp Epoch: 109 \tLoss: 0.025355\n",
      "Train fp Epoch: 110 \tLoss: 0.025334\n",
      "Train fp Epoch: 111 \tLoss: 0.025313\n",
      "Train fp Epoch: 112 \tLoss: 0.025293\n",
      "Train fp Epoch: 113 \tLoss: 0.025273\n",
      "Train fp Epoch: 114 \tLoss: 0.025254\n",
      "Train fp Epoch: 115 \tLoss: 0.025236\n",
      "Train fp Epoch: 116 \tLoss: 0.025219\n",
      "Train fp Epoch: 117 \tLoss: 0.025202\n",
      "Train fp Epoch: 118 \tLoss: 0.025186\n",
      "Train fp Epoch: 119 \tLoss: 0.025170\n",
      "Train fp Epoch: 120 \tLoss: 0.025155\n",
      "Train fp Epoch: 121 \tLoss: 0.025141\n",
      "Train fp Epoch: 122 \tLoss: 0.025127\n",
      "Train fp Epoch: 123 \tLoss: 0.025113\n",
      "Train fp Epoch: 124 \tLoss: 0.025100\n",
      "Train fp Epoch: 125 \tLoss: 0.025088\n",
      "Train fp Epoch: 126 \tLoss: 0.025075\n",
      "Train fp Epoch: 127 \tLoss: 0.025064\n",
      "Train fp Epoch: 128 \tLoss: 0.025053\n",
      "Train fp Epoch: 129 \tLoss: 0.025042\n",
      "Train fp Epoch: 130 \tLoss: 0.025031\n",
      "Train fp Epoch: 131 \tLoss: 0.025021\n",
      "Train fp Epoch: 132 \tLoss: 0.025011\n",
      "Train fp Epoch: 133 \tLoss: 0.025002\n",
      "Train fp Epoch: 134 \tLoss: 0.024993\n",
      "Train fp Epoch: 135 \tLoss: 0.024984\n",
      "Train fp Epoch: 136 \tLoss: 0.024976\n",
      "Train fp Epoch: 137 \tLoss: 0.024968\n",
      "Train fp Epoch: 138 \tLoss: 0.024960\n",
      "Train fp Epoch: 139 \tLoss: 0.024952\n",
      "Train fp Epoch: 140 \tLoss: 0.024945\n",
      "Train fp Epoch: 141 \tLoss: 0.024938\n",
      "Train fp Epoch: 142 \tLoss: 0.024931\n",
      "Train fp Epoch: 143 \tLoss: 0.024924\n",
      "Train fp Epoch: 144 \tLoss: 0.024918\n",
      "Train fp Epoch: 145 \tLoss: 0.024912\n",
      "Train fp Epoch: 146 \tLoss: 0.024906\n",
      "Train fp Epoch: 147 \tLoss: 0.024900\n",
      "Train fp Epoch: 148 \tLoss: 0.024895\n",
      "Train fp Epoch: 149 \tLoss: 0.024890\n",
      "Train fp Epoch: 150 \tLoss: 0.024885\n",
      "Train fp Epoch: 151 \tLoss: 0.024880\n",
      "Train fp Epoch: 152 \tLoss: 0.024875\n",
      "Train fp Epoch: 153 \tLoss: 0.024870\n",
      "Train fp Epoch: 154 \tLoss: 0.024866\n",
      "Train fp Epoch: 155 \tLoss: 0.024862\n",
      "Train fp Epoch: 156 \tLoss: 0.024858\n",
      "Train fp Epoch: 157 \tLoss: 0.024854\n",
      "Train fp Epoch: 158 \tLoss: 0.024850\n",
      "Train fp Epoch: 159 \tLoss: 0.024846\n",
      "Train fp Epoch: 160 \tLoss: 0.024842\n",
      "Train fp Epoch: 161 \tLoss: 0.024839\n",
      "Train fp Epoch: 162 \tLoss: 0.024836\n",
      "Train fp Epoch: 163 \tLoss: 0.024832\n",
      "Train fp Epoch: 164 \tLoss: 0.024829\n",
      "Train fp Epoch: 165 \tLoss: 0.024826\n",
      "Train fp Epoch: 166 \tLoss: 0.024824\n",
      "Train fp Epoch: 167 \tLoss: 0.024821\n",
      "Train fp Epoch: 168 \tLoss: 0.024818\n",
      "Train fp Epoch: 169 \tLoss: 0.024815\n",
      "Train fp Epoch: 170 \tLoss: 0.024813\n",
      "Train fp Epoch: 171 \tLoss: 0.024811\n",
      "Train fp Epoch: 172 \tLoss: 0.024808\n",
      "Train fp Epoch: 173 \tLoss: 0.024806\n",
      "Train fp Epoch: 174 \tLoss: 0.024804\n",
      "Train fp Epoch: 175 \tLoss: 0.024802\n",
      "Train fp Epoch: 176 \tLoss: 0.024800\n",
      "Train fp Epoch: 177 \tLoss: 0.024798\n",
      "Train fp Epoch: 178 \tLoss: 0.024796\n",
      "Train fp Epoch: 179 \tLoss: 0.024794\n",
      "Train fp Epoch: 180 \tLoss: 0.024792\n",
      "Train fp Epoch: 181 \tLoss: 0.024791\n",
      "Train fp Epoch: 182 \tLoss: 0.024789\n",
      "Train fp Epoch: 183 \tLoss: 0.024787\n",
      "Train fp Epoch: 184 \tLoss: 0.024786\n",
      "Train fp Epoch: 185 \tLoss: 0.024784\n",
      "Train fp Epoch: 186 \tLoss: 0.024783\n",
      "Train fp Epoch: 187 \tLoss: 0.024782\n",
      "Train fp Epoch: 188 \tLoss: 0.024780\n",
      "Train fp Epoch: 189 \tLoss: 0.024779\n",
      "Train fp Epoch: 190 \tLoss: 0.024778\n",
      "Train fp Epoch: 191 \tLoss: 0.024777\n",
      "Train fp Epoch: 192 \tLoss: 0.024775\n",
      "Train fp Epoch: 193 \tLoss: 0.024774\n",
      "Train fp Epoch: 194 \tLoss: 0.024773\n",
      "Train fp Epoch: 195 \tLoss: 0.024772\n",
      "Train fp Epoch: 196 \tLoss: 0.024771\n",
      "Train fp Epoch: 197 \tLoss: 0.024770\n",
      "Train fp Epoch: 198 \tLoss: 0.024769\n",
      "Train fp Epoch: 199 \tLoss: 0.024768\n",
      "Train fp Epoch: 200 \tLoss: 0.024767\n",
      "test fp MSE: 0.057853\n",
      "test fp MAE: 0.195157\n",
      "test fp Loss: 0.029418\n",
      "开始备份网络权重\n",
      "开始处理草药权重矩阵\n",
      "开始处理化合物权重矩阵\n",
      "\n",
      "开始处理基因权重矩阵\n",
      "\n",
      "开始处理通路权重矩阵\n",
      "\n",
      "开始处理功能权重矩阵\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "h = 20\n",
    "import torch\n",
    "from read_file import read_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "for i in range(0,n):\n",
    "    print(\"第%d个药方\"%(i+1))\n",
    "    y_train = Y[list1].iloc[i]\n",
    "    y_test = Y[list2].iloc[i]\n",
    "    trainset_fp = torch.utils.data.TensorDataset(torch.FloatTensor(np.array(X_train)), torch.FloatTensor(np.array(y_train)))\n",
    "    trainloader_fp = torch.utils.data.DataLoader(trainset_fp, batch_size=len(y_train), shuffle=True, num_workers=2)\n",
    "    testset_fp = torch.utils.data.TensorDataset(torch.FloatTensor(np.array(X_test)), torch.FloatTensor(np.array(y_test)))\n",
    "    testloader_fp = torch.utils.data.DataLoader(testset_fp, batch_size=len(y_test), shuffle=True, num_workers=2)\n",
    "    #备份第一次数据\n",
    "    print(\"完成数据切分\")\n",
    "    m1,m2,m3,m4,m5,m6,m7,m8,m9 = read_file(i)\n",
    "    print(\"完成数据读取，开始稀疏化网络\")\n",
    "    paras = list(network_fingerprints.parameters())\n",
    "    with torch.no_grad():\n",
    "        print(\"开始处理草药权重矩阵\")\n",
    "        for k in range(0,len(m1)):\n",
    "            paras[1][k] = m2_2[k]*m1[k][0]\n",
    "        print(\"开始处理化合物权重矩阵\")\n",
    "        for k in range(0,len(m3)):\n",
    "            paras[3][k] = m4_4[k]*m3[k][0]\n",
    "        print(\"\\n开始处理基因权重矩阵\")\n",
    "        for k in range(0,len(m5)):\n",
    "            paras[5][k] = m6_6[k]*m5[k][0]\n",
    "        print(\"\\n开始处理通路权重矩阵\")\n",
    "        for k in range(0,len(m7)):\n",
    "            paras[7][k] = m8_8[k]*m7[k][0]\n",
    "        print(\"\\n开始处理功能权重矩阵\")\n",
    "        for k in range(0,len(m9)):\n",
    "            paras[9][k] = m10_10[k]*m9[k][0]\n",
    "    print(\"开始训练和测试\")\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    n_epochs = 200\n",
    "    loss1 = []\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train(epoch, DEVICE)\n",
    "        aa = (loss.cpu()).detach().numpy()\n",
    "        loss1.append(aa)\n",
    "    loss1=pd.DataFrame(loss1)\n",
    "    loss1.to_csv(\"%d.csv\"%(i))\n",
    "    #test(DEVICE,i)\n",
    "    out = test(DEVICE,i)\n",
    "    df = pd.DataFrame(out)\n",
    "    df.to_csv(\"%d.csv\"%(h))\n",
    "    h = h+1\n",
    "    paras = list(network_fingerprints.parameters())\n",
    "    print(\"开始备份网络权重\")\n",
    "    with torch.no_grad():\n",
    "        print(\"开始处理草药权重矩阵\")\n",
    "        for k in range(0,paras[1].shape[0]):\n",
    "            if(paras[1][k] != 0):\n",
    "                m2_2[k] = paras[1][k].clone()\n",
    "        print(\"开始处理化合物权重矩阵\")\n",
    "        for k in range(0,paras[3].shape[0]):\n",
    "            if(paras[3][k] != 0):\n",
    "                m4_4[k] = paras[3][k].clone()\n",
    "        print(\"\\n开始处理基因权重矩阵\")\n",
    "        for k in range(0,paras[5].shape[0]):\n",
    "            if(paras[5][k] != 0):\n",
    "                m6_6[k] = paras[5][k].clone()\n",
    "        print(\"\\n开始处理通路权重矩阵\")\n",
    "        for k in range(0,paras[7].shape[0]):\n",
    "            if(paras[7][k] != 0):\n",
    "                m8_8[i] = paras[7][i].clone()\n",
    "        print(\"\\n开始处理功能权重矩阵\")\n",
    "        for k in range(0,paras[9].shape[0]):\n",
    "            if(paras[9][k] != 0):\n",
    "                m10_10[k] = paras[9][k].clone() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0169, -0.0195,  0.0229,  ..., -0.0049, -0.0282,  0.0212],\n",
       "         [ 0.0327, -0.0333,  0.0539,  ..., -0.0380, -0.0376, -0.0410],\n",
       "         [ 0.0223,  0.0275, -0.0289,  ...,  0.0388,  0.0146,  0.0042],\n",
       "         ...,\n",
       "         [-0.0320,  0.0273, -0.0006,  ..., -0.0026, -0.0479,  0.0214],\n",
       "         [ 0.0129,  0.0027,  0.0276,  ..., -0.0500,  0.0328,  0.0517],\n",
       "         [ 0.0239, -0.0281,  0.0201,  ..., -0.0458, -0.0004,  0.0084]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-1.6281e-04, -3.7707e-04, -3.8406e-03, -1.4018e-04, -6.5165e-05,\n",
       "         -2.8997e-04, -7.9929e-05, -1.5069e-04, -1.5647e-03, -2.1975e-05,\n",
       "         -3.8317e-04,  2.0882e-06, -6.7789e-05, -7.9984e-05, -1.0883e-05,\n",
       "         -6.6517e-03, -1.1587e-05, -7.4261e-06, -9.6829e-04, -9.7515e-03,\n",
       "         -2.7488e-03, -1.5591e-04, -1.2686e-04, -6.3524e-03, -3.9933e-04,\n",
       "         -3.9027e-03, -5.0979e-03, -1.7586e-03, -8.2502e-04, -4.9332e-03,\n",
       "         -1.0107e-02, -2.2639e-03,  3.4629e-04, -1.0256e-03, -6.5801e-04,\n",
       "         -7.8543e-04, -3.8927e-03, -2.0791e-03, -1.3868e-03,  1.8713e-04,\n",
       "         -3.6854e-03, -2.4888e-03,  2.4194e-04, -4.0253e-03, -2.1362e-04,\n",
       "         -3.4350e-04, -9.4776e-06, -3.2689e-04, -8.8107e-03, -1.4375e-03,\n",
       "         -6.0070e-05, -1.4061e-03, -8.0225e-03, -3.0136e-04, -1.8975e-03,\n",
       "         -1.9388e-03, -5.1097e-04, -1.6307e-06, -5.5736e-04, -9.1474e-04,\n",
       "         -7.2970e-04, -3.9461e-03, -8.4851e-03, -1.8618e-04, -1.4274e-04,\n",
       "          1.6450e-04, -1.3294e-04, -4.8222e-03, -4.4818e-03, -1.0925e-04,\n",
       "         -5.0818e-04, -1.7873e-05, -6.4594e-03, -3.1113e-04, -1.5180e-04,\n",
       "         -3.5416e-03, -6.4391e-05, -7.3205e-03, -1.8555e-02, -4.4077e-03,\n",
       "         -5.8156e-03, -2.0875e-04, -5.2921e-04, -3.6715e-04, -5.2235e-05,\n",
       "         -2.7062e-03, -8.5971e-04, -7.3353e-04, -4.0492e-03, -3.5324e-05,\n",
       "         -2.8573e-03, -5.3981e-04, -6.3078e-04,  1.9884e-06, -5.8423e-04,\n",
       "         -3.7250e-04, -3.5844e-03, -7.6457e-03, -1.1399e-04, -1.8170e-04,\n",
       "         -3.2558e-03, -1.1487e-05, -9.1249e-04, -2.6427e-03, -9.5540e-04,\n",
       "         -2.8395e-03, -3.1846e-04, -3.4999e-03, -4.2351e-03, -2.4676e-04,\n",
       "         -7.8546e-03, -7.3239e-03, -6.1577e-03, -1.0089e-03, -1.4827e-03,\n",
       "         -4.3018e-04, -2.3133e-03, -6.6538e-03, -1.0767e-03, -2.5127e-03,\n",
       "         -1.4203e-05, -6.1162e-06, -1.7812e-03, -3.1483e-04, -1.9249e-03,\n",
       "         -3.4644e-05, -4.5446e-05, -3.5656e-04, -6.6481e-03, -7.4768e-03,\n",
       "         -2.0884e-03, -1.5588e-04,  1.2701e-05, -1.4968e-03,  5.8366e-04,\n",
       "         -3.3730e-04, -7.7453e-06, -2.6659e-03,  2.1385e-04,  1.1545e-04,\n",
       "         -2.8699e-03, -3.1651e-03, -1.0947e-03, -1.3639e-05, -7.2767e-04,\n",
       "         -5.8796e-03, -7.3140e-03, -1.6126e-03, -1.5966e-03, -7.4211e-03,\n",
       "         -4.1191e-04, -7.1187e-03, -3.5022e-03, -1.6056e-04, -8.2774e-03,\n",
       "         -8.5241e-04, -4.3214e-04, -6.3576e-03,  4.7909e-04, -2.2422e-03,\n",
       "         -2.5833e-06, -5.4170e-03,  2.2997e-06, -1.8460e-04, -5.9317e-04,\n",
       "         -2.4917e-05, -4.8902e-03, -1.4554e-04, -4.4184e-05, -2.1418e-04,\n",
       "          9.0298e-04, -1.8203e-03, -2.4803e-05, -1.3985e-04, -9.2236e-04,\n",
       "         -1.0746e-04, -3.3875e-06, -3.2776e-03, -1.3432e-03, -3.3721e-06,\n",
       "         -3.0868e-03, -2.2269e-04, -2.1421e-06, -1.5764e-03, -1.4670e-03,\n",
       "         -2.1100e-04, -2.2532e-04, -4.9842e-05, -0.0000e+00, -8.6512e-03,\n",
       "         -6.3821e-04,  7.5940e-06, -7.3074e-04, -5.8091e-03, -3.5501e-05,\n",
       "         -1.7963e-03, -3.8559e-05, -2.5985e-03, -6.0999e-03, -1.1033e-02,\n",
       "         -2.2433e-03, -2.4942e-03, -2.0684e-03, -1.0135e-02], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0693, -0.0328,  0.0313,  ...,  0.0106,  0.0277,  0.0355],\n",
       "         [-0.0425,  0.0240, -0.0632,  ...,  0.0577,  0.0549, -0.0082],\n",
       "         [-0.0426, -0.0063, -0.0696,  ..., -0.0040,  0.0417,  0.0170],\n",
       "         ...,\n",
       "         [-0.0299, -0.0272,  0.0355,  ..., -0.0261,  0.0255,  0.0007],\n",
       "         [-0.0265, -0.0131, -0.0132,  ..., -0.0366,  0.0355,  0.0214],\n",
       "         [-0.0134, -0.0039, -0.0207,  ...,  0.0180,  0.0604, -0.0691]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 4.2471e-05, -1.9389e-05,  2.5463e-05,  ...,  4.7496e-05,\n",
       "         -2.7459e-04,  1.1242e-06], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 8.4002e-03, -6.6735e-04, -2.4001e-03,  ..., -7.3402e-04,\n",
       "           2.3928e-03,  5.6889e-03],\n",
       "         [ 2.3083e-03,  1.3737e-03,  5.6785e-03,  ...,  3.6927e-03,\n",
       "           6.8096e-03, -8.9875e-05],\n",
       "         [-2.5469e-03,  2.9595e-03,  8.3089e-03,  ...,  7.0765e-03,\n",
       "          -4.1074e-03,  6.4024e-03],\n",
       "         ...,\n",
       "         [-4.7731e-03,  4.4529e-03, -4.7829e-03,  ...,  4.5769e-03,\n",
       "           4.7154e-03,  6.5021e-03],\n",
       "         [ 3.1576e-03,  2.2240e-04, -1.8221e-03,  ...,  4.4995e-03,\n",
       "          -4.4964e-04, -2.4995e-03],\n",
       "         [-1.4760e-03,  3.5748e-03, -6.9630e-03,  ...,  8.0482e-03,\n",
       "          -2.9747e-03,  7.7108e-03]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 7.8117e-03,  2.2195e-03, -3.5209e-03,  ..., -1.0744e-03,\n",
       "         -8.0033e-04, -3.2607e-07], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0039,  0.0056, -0.0067,  ...,  0.0075, -0.0056, -0.0048],\n",
       "         [ 0.0037, -0.0084, -0.0047,  ...,  0.0050, -0.0041,  0.0085],\n",
       "         [ 0.0091,  0.0012, -0.0069,  ..., -0.0086, -0.0058,  0.0048],\n",
       "         ...,\n",
       "         [ 0.0077, -0.0035,  0.0083,  ...,  0.0043,  0.0063, -0.0051],\n",
       "         [-0.0048,  0.0089, -0.0011,  ...,  0.0075,  0.0066, -0.0024],\n",
       "         [-0.0089,  0.0006,  0.0047,  ..., -0.0052,  0.0018,  0.0018]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-4.7545e-05, -4.4364e-05,  2.7365e-05,  ..., -7.2215e-03,\n",
       "         -0.0000e+00, -8.1475e-04], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0066,  0.0058,  0.0011,  ..., -0.0018,  0.0013,  0.0019],\n",
       "         [ 0.0033, -0.0019, -0.0035,  ..., -0.0006,  0.0020, -0.0016],\n",
       "         [-0.0064,  0.0052, -0.0064,  ..., -0.0035, -0.0028,  0.0008],\n",
       "         ...,\n",
       "         [ 0.0040,  0.0046, -0.0015,  ..., -0.0049,  0.0063, -0.0046],\n",
       "         [-0.0033, -0.0019,  0.0057,  ..., -0.0035,  0.0023, -0.0004],\n",
       "         [ 0.0065, -0.0041,  0.0056,  ..., -0.0059, -0.0009,  0.0012]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0127,  0.0026, -0.0021,  ..., -0.0042, -0.0039,  0.0001],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0207,  0.0132,  0.0040,  ..., -0.0162, -0.0014,  0.0010]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.4580], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paras = list(network_fingerprints.parameters())\n",
    "paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.6281e-04, -3.7707e-04, -3.8406e-03, -1.4018e-04, -6.5165e-05,\n",
       "        -2.8997e-04, -7.9929e-05, -1.5069e-04, -1.5647e-03, -2.1975e-05,\n",
       "        -3.8317e-04,  2.0882e-06, -6.7789e-05, -7.9984e-05, -1.0883e-05,\n",
       "        -6.6517e-03, -1.1587e-05, -7.4261e-06, -9.6829e-04, -9.7515e-03,\n",
       "        -2.7488e-03, -1.5591e-04, -1.2686e-04, -6.3524e-03, -3.9933e-04,\n",
       "        -3.9027e-03, -5.0979e-03, -1.7586e-03, -8.2502e-04, -4.9332e-03,\n",
       "        -1.0107e-02, -2.2639e-03,  3.4629e-04, -1.0256e-03, -6.5801e-04,\n",
       "        -7.8543e-04, -3.8927e-03, -2.0791e-03, -1.3868e-03,  1.8713e-04,\n",
       "        -3.6854e-03, -2.4888e-03,  2.4194e-04, -4.0253e-03, -2.1362e-04,\n",
       "        -3.4350e-04, -9.4776e-06, -3.2689e-04, -8.8107e-03, -1.4375e-03,\n",
       "        -6.0070e-05, -1.4061e-03, -8.0225e-03, -3.0136e-04, -1.8975e-03,\n",
       "        -1.9388e-03, -5.1097e-04, -1.6307e-06, -5.5736e-04, -9.1474e-04,\n",
       "        -7.2970e-04, -3.9461e-03, -8.4851e-03, -1.8618e-04, -1.4274e-04,\n",
       "         1.6450e-04, -1.3294e-04, -4.8222e-03, -4.4818e-03, -1.0925e-04,\n",
       "        -5.0818e-04, -1.7873e-05, -6.4594e-03, -3.1113e-04, -1.5180e-04,\n",
       "        -3.5416e-03, -6.4391e-05, -7.3205e-03, -1.8555e-02, -4.4077e-03,\n",
       "        -5.8156e-03, -2.0875e-04, -5.2921e-04, -3.6715e-04, -5.2235e-05,\n",
       "        -2.7062e-03, -8.5971e-04, -7.3353e-04, -4.0492e-03, -3.5324e-05,\n",
       "        -2.8573e-03, -5.3981e-04, -6.3078e-04,  1.9884e-06, -5.8423e-04,\n",
       "        -3.7250e-04, -3.5844e-03, -7.6457e-03, -1.1399e-04, -1.8170e-04,\n",
       "        -3.2558e-03, -1.1487e-05, -9.1249e-04, -2.6427e-03, -9.5540e-04,\n",
       "        -2.8395e-03, -3.1846e-04, -3.4999e-03, -4.2351e-03, -2.4676e-04,\n",
       "        -7.8546e-03, -7.3239e-03, -6.1577e-03, -1.0089e-03, -1.4827e-03,\n",
       "        -4.3018e-04, -2.3133e-03, -6.6538e-03, -1.0767e-03, -2.5127e-03,\n",
       "        -1.4203e-05, -6.1162e-06, -1.7812e-03, -3.1483e-04, -1.9249e-03,\n",
       "        -3.4644e-05, -4.5446e-05, -3.5656e-04, -6.6481e-03, -7.4768e-03,\n",
       "        -2.0884e-03, -1.5588e-04,  1.2701e-05, -1.4968e-03,  5.8366e-04,\n",
       "        -3.3730e-04, -7.7453e-06, -2.6659e-03,  2.1385e-04,  1.1545e-04,\n",
       "        -2.8699e-03, -3.1651e-03, -1.0947e-03, -1.3639e-05, -7.2767e-04,\n",
       "        -5.8796e-03, -7.3140e-03, -1.6126e-03, -1.5966e-03, -7.4211e-03,\n",
       "        -4.1191e-04, -7.1187e-03, -3.5022e-03, -1.6056e-04, -8.2774e-03,\n",
       "        -8.5241e-04, -4.3214e-04, -6.3576e-03,  4.7909e-04, -2.2422e-03,\n",
       "        -2.5833e-06, -5.4170e-03,  2.2997e-06, -1.8460e-04, -5.9317e-04,\n",
       "        -2.4917e-05, -4.8902e-03, -1.4554e-04, -4.4184e-05, -2.1418e-04,\n",
       "         9.0298e-04, -1.8203e-03, -2.4803e-05, -1.3985e-04, -9.2236e-04,\n",
       "        -1.0746e-04, -3.3875e-06, -3.2776e-03, -1.3432e-03, -3.3721e-06,\n",
       "        -3.0868e-03, -2.2269e-04, -2.1421e-06, -1.5764e-03, -1.4670e-03,\n",
       "        -2.1100e-04, -2.2532e-04, -4.9842e-05, -2.0250e-02, -8.6512e-03,\n",
       "        -6.3821e-04,  7.5940e-06, -7.3074e-04, -5.8091e-03, -3.5501e-05,\n",
       "        -1.7963e-03, -3.8559e-05, -2.5985e-03, -6.0999e-03, -1.1033e-02,\n",
       "        -2.2433e-03, -2.4942e-03, -2.0684e-03, -1.0135e-02], device='cuda:0',\n",
       "       grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始处理草药权重矩阵\n",
      "开始处理化合物权重矩阵\n",
      "\n",
      "开始处理基因权重矩阵\n",
      "\n",
      "开始处理通路权重矩阵\n",
      "\n",
      "开始处理功能权重矩阵\n"
     ]
    }
   ],
   "source": [
    " with torch.no_grad():\n",
    "    print(\"开始处理草药权重矩阵\")\n",
    "    for i in range(0,paras[1].shape[0]):\n",
    "        paras[1][i] = m2_2[i]\n",
    "    print(\"开始处理化合物权重矩阵\")\n",
    "    for i in range(0,paras[3].shape[0]):\n",
    "        paras[3][i] = m4_4[i]\n",
    "    print(\"\\n开始处理基因权重矩阵\")\n",
    "    for i in range(0,paras[5].shape[0]):\n",
    "        paras[5][i] = m6_6[i]\n",
    "    print(\"\\n开始处理通路权重矩阵\")\n",
    "    for i in range(0,paras[7].shape[0]):\n",
    "        paras[7][i] = m8_8[i]\n",
    "    print(\"\\n开始处理功能权重矩阵\")\n",
    "    for i in range(0,paras[9].shape[0]):\n",
    "        paras[9][i] = m10_10[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(network_fingerprints, \"H:\\工作记录\\中医药\\数据集2\\entire_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
